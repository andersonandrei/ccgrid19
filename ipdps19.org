# -*- mode: org -*-
# -*- coding: utf-8 -*-
#+STARTUP: overview indent inlineimages logdrawer

#+TITLE: Autotuning under Tight Budget Constraints: @@latex: \\@@ A Transparent Design of Experiments Approach
#+LANGUAGE:    en
#+TAGS: noexport(n) Stats(S)
#+TAGS: Teaching(T) R(R) OrgMode(O) Python(P)
#+TAGS: Book(b) DOE(D) Code(C) NODAL(N) FPGA(F) Autotuning(A) Arnaud(r)
#+TAGS: DataVis(v) PaperReview(W)
#+EXPORT_SELECT_TAGS: Blog
#+OPTIONS:   H:3 num:t toc:nil \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:nil skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+COLUMNS: %25ITEM %TODO %3PRIORITY %TAGS
#+SEQ_TODO: TODO(t!) STARTED(s!) WAITING(w@) APPT(a!) | DONE(d!) CANCELLED(c!) DEFERRED(f!)

#+LATEX_CLASS: org-ieeetran
#+LATEX_CLASS_OPTIONS: [conference]
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage{amssymb}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{xcolor}
#+LATEX_HEADER: \usepackage{url}
#+LATEX_HEADER: \usepackage{listings}
#+LATEX_HEADER: %\usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage[english]{babel}
#+LATEX_HEADER: \usepackage{multirow}
#+LATEX_HEADER: \usepackage{caption}
#+LATEX_HEADER: \usepackage{hyperref}
#+LATEX_HEADER: \usepackage{booktabs}
#+LATEX_HEADER: \usepackage{array}
#+LATEX_HEADER: \usepackage{relsize}
#+LATEX_HEADER: \usepackage{bm}
#+LATEX_HEADER: \usepackage{wasysym}
#+LATEX_HEADER: \usepackage{ragged2e}

#+LATEX_HEADER: \lstset{ %
#+LATEX_HEADER:   backgroundcolor={},
#+LATEX_HEADER:   basicstyle=\ttfamily\scriptsize,
#+LATEX_HEADER:   breakatwhitespace=true,
#+LATEX_HEADER:   breaklines=true,
#+LATEX_HEADER:   captionpos=n,
#+LATEX_HEADER:   extendedchars=true,
#+LATEX_HEADER:   frame=n,
#+LATEX_HEADER:   rulecolor=\color{black},
#+LATEX_HEADER:   showspaces=false,
#+LATEX_HEADER:   showstringspaces=false,
#+LATEX_HEADER:   showtabs=false,
#+LATEX_HEADER:   stepnumber=2,
#+LATEX_HEADER:   stringstyle=\color{gray},
#+LATEX_HEADER:   tabsize=2,
#+LATEX_HEADER: }
#+LATEX_HEADER: \renewcommand*{\UrlFont}{\ttfamily\smaller\relax}

#+LATEX_HEADER: \graphicspath{{./img/}}
#+LATEX_HEADER: \renewcommand*{\UrlFont}{\ttfamily\smaller\relax}

#+LATEX_HEADER: \author{\IEEEauthorblockN{Pedro Bruel\IEEEauthorrefmark{1}\IEEEauthorrefmark{2},
#+LATEX_HEADER: Arnaud Legrand\IEEEauthorrefmark{1},
#+LATEX_HEADER: Brice Videau\IEEEauthorrefmark{1},
#+LATEX_HEADER: Jean-Marc Vincent\IEEEauthorrefmark{1}, and
#+LATEX_HEADER: Alfredo Goldman\IEEEauthorrefmark{2}}
#+LATEX_HEADER: \IEEEauthorblockA{\IEEEauthorrefmark{1}University of Grenoble Alpes, CNRS, INRIA, LIG - Grenoble, France\\
#+LATEX_HEADER: Email: \{arnaud.legrand, brice.videau, jean-marc.vincent\}@imag.fr}
#+LATEX_HEADER: \IEEEauthorblockA{\IEEEauthorrefmark{2}University of São Paulo - São Paulo, Brazil\\
#+LATEX_HEADER: Email: \{phrb, gold\}@ime.usp.br}}

#+LATEX: \begin{abstract}
A large quantity of resources is spent writing, porting, and optimizing
scientific and industrial High Performance Computing applications. Autotuning
techniques have become therefore fundamental to lower the costs of leveraging
the improvements on execution time and power consumption provided by the latest
software and hardware platforms. Despite this, the most popular autotuning
techniques still require a large budget of costly experimental measurements to
provide good results while still not providing exploitable knowledge about the
problem after optimization. In this paper we present a user-transparent
autotuning technique based on Design of Experiments that is capable of operating
under tight budget constraints by significantly reducing the amount of
measurements needed to find good optimizations. Our approach also enable users
to make informed decisions on what optimizations to pursue and when to stop
optimizing. We present experimental evaluations of our approach and show that,
leveraging user decisions, it is capable of finding the global optimum of a GPU
Laplacian kernel optimization using half of the measurement budget used by other
common autotuning techniques. We also show that our approach is capable of using
generic performance models to decrease the measurement budget needed to find
speedups of up to 50$\times$, when compared to random sampling, for some
applications from a more comprehensive autotuning benchmark.
#+LATEX: \end{abstract}

* Arnaud's Draft                                                   :noexport:
** Intro
** Context
- HPC, optimizing code is a nightmare although very important gains
  can be expected when one can afford an expert to work on it.
- Typical techniques are source-to-source transformation + compiler
  flag optimization
- Even when automatic, this optimization can be very time consumming
  (costly experiments + curse of dimensionality).
** Related Work
*** Source-to-source transformation
*** Auto-tuning frameworks
*** Exploration Strategies
** Statement
- Generic Meta-Heuristics (GAs, Simulated Annealing, Tabu Search) do
  not exploit well specific properties of the problem and require very
  large amount of measurements.
- Classical Mathematical Optimization techniques (gradient, surrogate,
  ...) are ineffective in this context as the geometry is far more
  complicated than what can be found in maths textbooks
- Fully automatic ML make sense to model and predict important factors
  but typically require a large amount of data to be effective as the
  class of underlying models is generally very large.
- In many settings a naive uniform random sampling strategy works just
  as well as other methods.
- None of the above methods really brings exploitable knowledge
  allowing to decide whether further exploration may be useful.
** Proposal
Sequential approach, using D-optimal designs. Requires a model
(ideally provided by an expert) which is iteratively refined.
*** D-optimal designs in a nutshell
- Explanations of DoE + Simple illustration
- Analysis strategy (aov, lm)
- Allows a global overview and to detect the main factors right away
  to focus on the most promising parts of the subspace
- This assumes that there is a global geometry of the problem that can
  be exploited despite the roughness of the local geometry. This
  assumption may be wrong but is likely to go detected.
*** General Method in the context of auto-tuning
Ideally, human in the loop but for the sake of a general performance
evaluation, we had to automate it.
** Performance Evaluation
*** Experimental Methodology
G5K, database, RR, R + julia +...
*** Working out a simple example in details: a Laplacian Kernel
Laplacian Kernel on a GPU + BOAST
*** Evaluation on the ??? benchmark suite
ORIO
** Conclusion and Future Work
- DoE based strategy
- Revealed impressively effective for the Laplacian kernel.
- Not as impressive on the other benchmarks but despite their general
  use, it apears that little gain can be expected. In any cases, our
  approach produces at least as good results with far fewer measurements.
- Future work:
  - Other benchmarks
  - source-to-source + compiler flags
  - connexion with online learning
* Rosenbrock Example Setup                                         :noexport:
#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(AlgDesign)

rosenbrock <- function(x, y) {
    return(((1.0 - x) ^ 2) + (100.0 * ((y - (x ^ 2)) ^ 2)))
}

rosenbrock_data <- expand.grid(seq(-4, 4, 0.05), seq(-4, 4, 0.05))
rosenbrock_data$Y <- mapply(rosenbrock, rosenbrock_data$Var1, rosenbrock_data$Var2)

dim(rosenbrock_data)
rosenbrock(1, 1)
#+END_SRC

#+RESULTS:
: [1] 25921     3
: [1] 0

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
random_sample <- rosenbrock_data[sample(1:nrow(rosenbrock_data), 8, replace = TRUE), ]
dim(random_sample)
random_sample[random_sample$Y == min(random_sample$Y), ]
#+END_SRC

#+RESULTS:
: [1] 8 3
:       Var1 Var2      Y
: 16666  0.1 1.15 130.77

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
sampled_mins <- replicate(100, {
  random_sample <- rosenbrock_data[sample(1:nrow(rosenbrock_data), 10, replace = TRUE), ]
  sample_min <- random_sample[random_sample$Y == min(random_sample$Y), "Y"]
  sample_min
  })

sampled_mins <- as.numeric(unlist(sampled_mins))
random_summary <- summary(sampled_mins)
random_summary
#+END_SRC

#+RESULTS:
:     Min.  1st Qu.   Median     Mean  3rd Qu.     Max.
:    1.153   10.954   43.828  133.926  130.590 1529.620

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
i <- 0
linear_mins <- replicate(100, {
  output <- optFederov(~ ., data = rosenbrock_data, nTrials = 10)
  regression <- lm(Y ~ ., data = output$design)
  prediction <- predict(regression, rosenbrock_data)
  best_predicted <- rosenbrock_data[prediction == min(prediction), ]
  i <<- i + 1
  print(i)
  best_predicted$Y
})

linear_mins <- as.numeric(unlist(linear_mins))
linear_summary <- summary(linear_mins)
linear_summary
#+END_SRC

#+RESULTS:
#+begin_example
[1] 1
[1] 2
[1] 3
[1] 4
[1] 5
[1] 6
[1] 7
[1] 8
[1] 9
[1] 10
[1] 11
[1] 12
[1] 13
[1] 14
[1] 15
[1] 16
[1] 17
[1] 18
[1] 19
[1] 20
[1] 21
[1] 22
[1] 23
[1] 24
[1] 25
[1] 26
[1] 27
[1] 28
[1] 29
[1] 30
[1] 31
[1] 32
[1] 33
[1] 34
[1] 35
[1] 36
[1] 37
[1] 38
[1] 39
[1] 40
[1] 41
[1] 42
[1] 43
[1] 44
[1] 45
[1] 46
[1] 47
[1] 48
[1] 49
[1] 50
[1] 51
[1] 52
[1] 53
[1] 54
[1] 55
[1] 56
[1] 57
[1] 58
[1] 59
[1] 60
[1] 61
[1] 62
[1] 63
[1] 64
[1] 65
[1] 66
[1] 67
[1] 68
[1] 69
[1] 70
[1] 71
[1] 72
[1] 73
[1] 74
[1] 75
[1] 76
[1] 77
[1] 78
[1] 79
[1] 80
[1] 81
[1] 82
[1] 83
[1] 84
[1] 85
[1] 86
[1] 87
[1] 88
[1] 89
[1] 90
[1] 91
[1] 92
[1] 93
[1] 94
[1] 95
[1] 96
[1] 97
[1] 98
[1] 99
[1] 100
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
  14409   14409   14409   14416   14425   14425
#+end_example

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
i <- 0
simple_model_mins <- replicate(10, {
  output <- optFederov(~ Var1 + Var2 + I(Var1 ^ 2) + I(Var2 ^ 2), data = rosenbrock_data, nTrials = 8)
  regression <- lm(Y ~ Var1 + Var2 + I(Var1 ^ 2) + I(Var2 ^ 2), data = output$design)
  prediction <- predict(regression, rosenbrock_data)
  best_predicted <- rosenbrock_data[prediction == min(prediction), ]
  i <<- i + 1
  print(i)
  best_predicted$Y
})

simple_model_mins <- as.numeric(unlist(simple_model_mins))
simple_model_summary <- summary(simple_model_mins)
simple_model_summary
#+END_SRC

#+RESULTS:
#+begin_example
[1] 1
[1] 2
[1] 3
[1] 4
[1] 5
[1] 6
[1] 7
[1] 8
[1] 9
[1] 10
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
   1583    1583    1588    1588    1593    1593
#+end_example

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
i <- 0
modelled_mins <- replicate(100, {
  output <- optFederov(~ Var1 + Var2 + I(Var1 ^ 4) + I(Var1 ^ 2) + I(Var2 ^ 2) + I(Var1 ^ 2):Var2, data = rosenbrock_data, nTrials = 8)
  regression <- lm(Y ~ Var1 + Var2 + I(Var1 ^ 4) + I(Var1 ^ 2) + I(Var2 ^ 2) + I(Var1 ^ 2):Var2, data = output$design)
  summary.aov(regression)
  prediction <- predict(regression, rosenbrock_data)
  best_predicted <- rosenbrock_data[prediction == min(prediction), ]
  i <<- i + 1
  print(i)
  best_predicted$Y
})

modelled_mins <- as.numeric(unlist(modelled_mins))
modelled_summary <- summary(modelled_mins)
modelled_summary
#+END_SRC

#+RESULTS:
#+begin_example
[1] 1
[1] 2
[1] 3
[1] 4
[1] 5
[1] 6
[1] 7
[1] 8
[1] 9
[1] 10
[1] 11
[1] 12
[1] 13
[1] 14
[1] 15
[1] 16
[1] 17
[1] 18
[1] 19
[1] 20
[1] 21
[1] 22
[1] 23
[1] 24
[1] 25
[1] 26
[1] 27
[1] 28
[1] 29
[1] 30
[1] 31
[1] 32
[1] 33
[1] 34
[1] 35
[1] 36
[1] 37
[1] 38
[1] 39
[1] 40
[1] 41
[1] 42
[1] 43
[1] 44
[1] 45
[1] 46
[1] 47
[1] 48
[1] 49
[1] 50
[1] 51
[1] 52
[1] 53
[1] 54
[1] 55
[1] 56
[1] 57
[1] 58
[1] 59
[1] 60
[1] 61
[1] 62
[1] 63
[1] 64
[1] 65
[1] 66
[1] 67
[1] 68
[1] 69
[1] 70
[1] 71
[1] 72
[1] 73
[1] 74
[1] 75
[1] 76
[1] 77
[1] 78
[1] 79
[1] 80
[1] 81
[1] 82
[1] 83
[1] 84
[1] 85
[1] 86
[1] 87
[1] 88
[1] 89
[1] 90
[1] 91
[1] 92
[1] 93
[1] 94
[1] 95
[1] 96
[1] 97
[1] 98
[1] 99
[1] 100
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
      0       0       0       0       0       0
#+end_example

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(dplyr)

summaries <- as.data.frame(rbind(random_summary, linear_summary, modelled_summary))
summaries <- summaries[, c("Min.", "Mean", "Max.")]
summaries$Method <- c("Random Sampling", "D-Opt. w/ Linear Model", "D-Opt. w/ Correct Model")
summaries <- summaries[, c("Method", "Mean", "Min.", "Max.")]
rownames(summaries) <- NULL
summaries
write.csv(summaries, file = "data/rosenbrock_summaries.csv", row.names = FALSE)
#+END_SRC

#+RESULTS:
:                    Method       Mean         Min.     Max.
: 1         Random Sampling   133.9259     1.153125  1529.62
: 2  D-Opt. w/ Linear Model 14415.8800 14409.000000 14425.00
: 3 D-Opt. w/ Correct Model     0.0000     0.000000     0.00

* Generating Figures                                               :noexport:
** SPAPT
*** Cloning/Pulling the Repository
#+HEADER: :results output :eval no-export
#+BEGIN_SRC shell
git clone https://github.com/phrb/dlmt_spapt_experiments.git || (cd dlmt_spapt_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.

*** Generate pdf
#+HEADER: :results graphics output :session *R* :eval no-export
#+HEADER: :file ./img/iteration_best_comparison.pdf
#+HEADER: :width 11 :height 16
#+BEGIN_SRC R
library(ggplot2)
library(plyr)
library(dplyr)
library(reshape2)
library(openssl)
library(RColorBrewer)
library(extrafont)

data_dir <- "dlmt_spapt_experiments/data/results"
target_dirs <- list.dirs(path = data_dir, full.names = FALSE, recursive = FALSE)
data <- NULL

read.csv.iterations.cost <- function(csv_file) {
    data <- read.csv(csv_file, header = TRUE)

    data$experiment_id <- rep(sha1(csv_file), nrow(data))
    data_baseline <- data[data$baseline == "True", "cost_mean"]
    data$cost_baseline <- rep(data_baseline, nrow(data))
    data$speedup <- data_baseline / data$cost_mean
    data$max_run_speedup <- rep(max(data$speedup), nrow(data))
    data$min_run_cost <- rep(min(data$cost_mean), nrow(data))

    data <- data[data$baseline == "False", ]
    data <- data[data$correct_result == "True", ]

    data$best_iteration <- rep(as.numeric(rownames(data[data$speedup == max(data$speedup), ])), nrow(data))
    data$points <- rep(nrow(data), nrow(data))

    return(data)
}

for (target_dir in target_dirs) {
    target_path <- paste(data_dir, "/", target_dir, "/", sep = "")

    csv_files <- list.files(path = target_path, pattern = "search_space.csv", recursive = TRUE)
    if (length(csv_files) != 0) {
        csv_files <- paste0(target_path, csv_files)

        info <- file.info(csv_files)
        non_empty <- rownames(info[info$size != 0, ])
        csv_files <- csv_files[csv_files %in% non_empty]
        target_data <- lapply(csv_files, read.csv.iterations.cost)
        target_data <- bind_rows(target_data)
        target_data <- target_data[, c("cost_mean", "experiment_id", "technique", "cost_baseline", "min_run_cost", "best_iteration")]
        target_data$application <- rep(target_dir, nrow(target_data))

        if (is.null(data)) {
            data <- target_data
        } else {
            data <- bind_rows(data, target_data)
        }
    }
}

plot_data <- data %>%
             distinct(experiment_id, .keep_all = TRUE) %>%
             group_by(application) %>%
             mutate(mean_cost_baseline = mean(cost_baseline)) %>%
             ungroup()

rs_sample <- data[data$technique == "RS", c("application", "technique", "cost_mean")]
dlmt_sample <- data[data$technique == "DLMT", c("application", "technique", "cost_mean")]

ggplot(plot_data, aes(min_run_cost, best_iteration, color = technique)) +
    facet_wrap(application ~ ., ncol = 2) +
    geom_jitter(data = rs_sample, aes(x = cost_mean, y = 300), pch = 19, alpha = 0.1, height = 85, width = 0) +
    geom_jitter(data = dlmt_sample, aes(x = cost_mean, y = 100), pch = 19, alpha = 0.1, height = 85, width = 0) +
    geom_point(size = 2, pch = 19) +
    stat_ellipse(type = "t", linetype = 13) +
    geom_vline(aes(xintercept = mean_cost_baseline), linetype = 8, color = "black") +
    scale_x_continuous(trans = "log10") +
    #coord_flip() +
    ggtitle("") +
    ylab("Iteration where Best was Found") +
    xlab("Best Cost") +
    theme_bw(base_size = 14) +
    theme(legend.position = "bottom",
          legend.direction = "horizontal",
          legend.title = element_blank(),
          text = element_text(family="Noto Serif"),
          strip.background = element_rect(fill = "white"),
          plot.margin = unit(c(0.2, 0.2, 0.2, 0.2), "cm"))  +
    scale_color_brewer(palette = "Set1")
#+END_SRC

#+RESULTS:
[[file:./img/iteration_best_comparison.pdf]]
** Rosenbrock
#+HEADER: :results graphics output :session *R* :exports none :eval no-export
#+HEADER: :file "./img/rosenbrock.pdf"
#+HEADER: :width 12 :height 12
#+BEGIN_SRC R
library(ggplot2)

rosenbrock <- function(x, y) {
    return(((1.0 - x) ^ 2) + (100.0 * ((y - (x ^ 2)) ^ 2))+ rnorm(1, sd = 10))
}

rosenbrock_data <- expand.grid(seq(-4, 4, 0.05),
                               seq(-4, 4, 0.05))

names(rosenbrock_data) <- c("x", "y")
rosenbrock_data$Y <- mapply(rosenbrock,
                            rosenbrock_data$x,
                            rosenbrock_data$y)

ggplot(rosenbrock_data, aes(x, y, z = Y)) +
      scale_x_continuous(limits = c(-4, 4), expand = c(0, 0)) +
      scale_y_continuous(limits = c(-4, 4), expand = c(0, 0)) +
      #geom_contour(colour = "black", show.legend = FALSE, breaks = 5 * (10 ^ (-1:7))) +
      geom_point(size = 4, colour = "black", pch = 19, data = rosenbrock_data[rosenbrock_data$Y == min(rosenbrock_data$Y), ]) +
      geom_label(size = 11, colour = "black", data = rosenbrock_data[rosenbrock_data$Y == min(rosenbrock_data$Y), ], aes(x = x, y = y + 0.35, label = "rosenbrock(1, 1) = 0")) +
      theme_bw(base_size = 35) +
      theme(panel.grid = element_blank(), panel.border = element_blank())
#+END_SRC

#+RESULTS:
[[file:./img/rosenbrock.pdf]]
** GPU Laplacian Kernel
#+HEADER: :file ./img/comparison_histogram.pdf :exports none :width 7 :height 8 :eval no-export
#+BEGIN_SRC R :results output graphics  :session *R*
library(ggplot2)
library(plyr)

df_all_methods <- read.csv("./data/complete_1000.csv", strip.white = T, header = T)
df_all_methods$method <- factor(df_all_methods$method, levels = c("RS","LHS","GS","GSR","GA","LM", "LMB", "LMBT", "RQ", "DOPT", "DLM", "DLMT"))
df_all_methods <- subset(df_all_methods, method %in% c("RS", "LHS", "GS", "GSR", "GA", "LM", "DLMT"))

df_mean = ddply(df_all_methods,.(method), summarize,
                mean = mean(slowdown))

df_median = ddply(df_all_methods,.(method), summarize,
                  median = median(slowdown))

df_err = ddply(df_all_methods,.(method), summarize,
              mean = mean(slowdown), err = 2 * sd(slowdown) / sqrt(length(slowdown)))

df_max = ddply(df_all_methods,.(method), summarize, max = max(slowdown))

ggplot(df_all_methods) +
    facet_grid(method ~ .) +
    theme_bw(base_size = 18) +
    coord_cartesian(xlim = c(.9, 4), ylim = c(0, 1000)) +
    geom_histogram(aes(slowdown), binwidth = .05, fill = "gray48") +
    scale_y_continuous(breaks = c(0, 1000), labels = c("0", "1000")) +
    geom_curve(data = df_max, aes(x = max + .1, y = 500, xend = max, yend = 5), arrow = arrow(length = unit(0.05, "npc")), curvature = 0.3) +
    geom_text(aes(x = max+.2, y = 550, label = "max"), data = df_max) +
    geom_rect(data = df_err, aes(xmin = mean-err, xmax = mean + err, ymin = 0, ymax = 1000, fill = "red"), alpha = 0.3) +
    geom_vline(aes(xintercept = median), df_median, color = "darkgreen", linetype = 3) +
    geom_vline(aes(xintercept = mean), df_mean, color = "red", linetype = 2) +
    labs(y = "Frequency", x = "Slowdown compared to the optimal solution") +
    scale_fill_discrete(name = "", breaks = c("red"), labels = c("Mean error")) +
    ggtitle("") +
    theme(legend.position = "none",
          text = element_text(family="Noto Serif"),
          strip.background = element_rect(fill = "white"))
#+END_SRC

#+RESULTS:
[[file:./img/comparison_histogram.pdf]]
** Representing Sampling Strategies
*** Generate Fake Data with Algorithms
#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
fake_gradient_data_seed <- data.frame(x1 = c(1, 1, 99, 99),
                                      x2 = c(1, 99, 1, 99),
                                      run = c(1, 2, 3, 4),
                                      sign1 = c(1, 1, -1, -1),
                                      sign2 = c(1, -1, 1, -1))

fake_gradient_data <- NULL

for(run_id in c(1, 2, 3, 4)) {
  if (is.null(fake_gradient_data)) {
      fake_gradient_data <- fake_gradient_data_seed[run_id, ]
  } else {
      fake_gradient_data <- rbind(fake_gradient_data, fake_gradient_data_seed[run_id, ])
  }

  for(i in 1:10) {
      row <- nrow(fake_gradient_data)
      fake_descent <- data.frame(x1 = ceiling(fake_gradient_data[row, "x1"] + (fake_gradient_data[row, "sign1"] * runif(1, min = 1, max = 5))),
                                 x2 = ceiling(fake_gradient_data[row, "x2"] + (fake_gradient_data[row, "sign2"] * runif(1, min = 1, max = 5))),
                                 run = fake_gradient_data[row, "run"],
                                 sign1 = fake_gradient_data[row, "sign1"],
                                 sign2 = fake_gradient_data[row, "sign2"])
      fake_gradient_data <- rbind(fake_gradient_data, fake_descent)
  }
}

fake_gradient_data$name <- rep("Gradient Descent", nrow(fake_gradient_data))
data <- bind_rows(data, fake_gradient_data)

fake_sima_data_seed <- data.frame(x1 = c(30, 30, 70, 70),
                                  x2 = c(30, 70, 30, 70),
                                  run = c(1, 2, 3, 4),
                                  sign1 = c(1, 1, -1, -1),
                                  sign2 = c(1, -1, 1, -1))

fake_sima_data <- NULL

for(run_id in c(1, 2, 3, 4)) {
  if (is.null(fake_sima_data)) {
      fake_sima_data <- fake_sima_data_seed[run_id, ]
  } else {
      fake_sima_data <- rbind(fake_sima_data, fake_sima_data_seed[run_id, ])
  }

  for(i in 1:10) {
      row <- nrow(fake_sima_data)
      fake_descent <- data.frame(x1 = ceiling(fake_sima_data[row, "x1"] + (fake_sima_data[row, "sign1"] * runif(1, min = -5, max = 5))),
                                 x2 = ceiling(fake_sima_data[row, "x2"] + (fake_sima_data[row, "sign2"] * runif(1, min = -5, max = 5))),
                                 run = fake_sima_data[row, "run"],
                                 sign1 = fake_sima_data[row, "sign1"],
                                 sign2 = fake_sima_data[row, "sign2"])
      fake_sima_data <- rbind(fake_sima_data, fake_descent)
  }
}

fake_sima_data$name <- rep("Simulated Annealing", nrow(fake_sima_data))
data <- bind_rows(data, fake_sima_data)
#+END_SRC
*** Generate Data
#+HEADER: :results output :session *R* :eval no-export
#+BEGIN_SRC R
library(ggplot2)
library(DoE.wrapper)
library(AlgDesign)
library(dplyr)
library(RColorBrewer)

sample_size <- 50
pre_sample_size <- 30 * sample_size
search_space_size <- 100

center_x1 <- (search_space_size / 2) - 30
center_x2 <- (search_space_size / 2) - 30

get_cost <- function(data) {
    return(((data$x1 - center_x1) ^ 2) + ((data$x2 - center_x2) ^ 2))
}

objective_data <- expand.grid(seq(0, search_space_size, 1),
                              seq(0, search_space_size, 1))
names(objective_data) <- c("x1", "x2")

sima_samples <- 15

dev.off()

plot(x = c(0, 100, center_x1, 100, 0), y = c(0, 100, center_x2, 0, 100))
fake_sima_data <- as.data.frame(locator(n = sima_samples, type = "l"))
names(fake_sima_data) <- c("x1", "x2")
dev.off()

fake_sima_data$run <- c(rep(1, nrow(fake_sima_data)))
fake_sima_data$name <- rep("Simulated Annealing", nrow(fake_sima_data))

fake_sima_data$cost <- get_cost(fake_sima_data)
fake_sima_data$min <- fake_sima_data$cost == min(fake_sima_data$cost)

data <- fake_sima_data

descent_samples <- 20

plot(x = c(0, 100, center_x1, 100, 0), y = c(0, 100, center_x2, 0, 100))
fake_descent_data <- as.data.frame(locator(n = descent_samples, type = "l"))
names(fake_descent_data) <- c("x1", "x2")
dev.off()

paths <- 5
fake_runs <- rep(1, descent_samples / paths)
for(i in 2:paths){
  fake_runs <- c(fake_runs, rep(i, descent_samples / paths))
}

fake_descent_data$run <- fake_runs
fake_descent_data$name <- rep("Gradient Descent", nrow(fake_descent_data))

fake_descent_data$cost <- get_cost(fake_descent_data)
fake_descent_data$min <- fake_descent_data$cost == min(fake_descent_data$cost)

data <- bind_rows(data, fake_descent_data)

objective_data$Y <- get_cost(objective_data)

rs_data <- data.frame(x1 = sample(0:search_space_size, sample_size, replace = T),
                      x2 = sample(0:search_space_size, sample_size, replace = T))
rs_data$name <- rep("Random Sampling", nrow(rs_data))

rs_data$cost <- get_cost(rs_data)
rs_data$min <- rs_data$cost == min(rs_data$cost)

data <- bind_rows(data, rs_data)

lhs_data <- lhs.design(nruns = sample_size, nfactors = 2, digits = 0, type = "maximin",
                       factor.names = list(x1 = c(0, search_space_size), x2 = c(0, search_space_size)))
lhs_data$name <- rep("Latin Hypercube Sampling", nrow(lhs_data))

lhs_data$cost <- get_cost(lhs_data)
lhs_data$min <- lhs_data$cost == min(lhs_data$cost)

data <- bind_rows(data, lhs_data)

full_factorial <- gen.factorial(c(search_space_size, search_space_size), center = FALSE)
names(full_factorial) <- c("x1", "x2")
output <- optFederov(~ x1 + x2, full_factorial, nTrials = sample_size)
dopt_data <- output$design

dopt_data$name <- rep("DOpt. Linear Model", nrow(dopt_data))
dopt_data$cost <- get_cost(dopt_data)
dopt_data$min <- rep(FALSE, nrow(dopt_data))

regression <- lm(cost ~ x1 + x2, data = dopt_data)
prediction <- predict(regression, newdata = full_factorial)
best <- full_factorial[prediction == min(prediction), ]

best$cost <- min(prediction)
best$name <- "DOpt. Linear Model"
best$min <- TRUE

dopt_data <- bind_rows(dopt_data, best)
data <- bind_rows(data, dopt_data)

full_factorial <- gen.factorial(c(search_space_size, search_space_size), center = FALSE)
names(full_factorial) <- c("x1", "x2")
output <- optFederov(~ x1 + x2 + I(x1 ^ 2) + I(x2 ^ 2), full_factorial, nTrials = sample_size)
doptq_data <- output$design

doptq_data$name <- rep("DOpt. Quadratic Model", nrow(doptq_data))
doptq_data$cost <- get_cost(doptq_data)
doptq_data$min <- rep(FALSE, nrow(doptq_data))

regression <- lm(cost ~ x1 + x2 + I(x1 ^ 2) + I(x2 ^ 2), data = doptq_data)
prediction <- predict(regression, newdata = full_factorial)
best <- full_factorial[prediction == min(prediction), ]

best$cost <- min(prediction)
best$name <- "DOpt. Quadratic Model"
best$min <- TRUE

doptq_data <- bind_rows(doptq_data, best)
data <- bind_rows(data, doptq_data)
#+END_SRC

#+RESULTS:
: Error in dev.off() : cannot shut down device 1 (the null device)
: null device
:           1
: null device
:           1

*** Plot
#+HEADER: :results graphics output :session *R*
#+HEADER: :file ./img/sampling_comparison.pdf :exports none :width 15 :height 11.5 :eval no-export
#+BEGIN_SRC R
  library(extrafont)
  data$facet <- factor(data$name, levels = c("Random Sampling", "Latin Hypercube Sampling", "Gradient Descent", "Simulated Annealing", "DOpt. Linear Model", "DOpt. Quadratic Model"))
  ggplot(data, aes(x = x1, y = x2)) +
      scale_x_continuous(limits = c(-1, 101), expand = c(0, 0)) +
      scale_y_continuous(limits = c(-1, 101), expand = c(0, 0)) +
      xlab("x") +
      ylab("y") +
      facet_wrap(facet ~ ., ncol = 3) +
      #geom_raster(data = objective_data, aes(fill = Y), show.legend = FALSE) +
      #geom_contour(data = objective_data, aes(z = Y), colour = "white", linetype = 8) + #, breaks = 1 * (2 ^ (2:20))) +
      geom_contour(data = objective_data, aes(z = Y), linetype = 1, colour = "black", alpha = 0.6, show.legend = FALSE, breaks = 1 * (2 ^ (4:20))) +
      geom_path(data = subset(data, name %in% c("Gradient Descent", "Simulated Annealing")), aes(group = run), color = "black", alpha = 0.55, size = 1) +
      geom_point(shape = 19, size = 3, colour = "black", alpha = 0.55) +
      geom_jitter(data = subset(data, name %in% c("Gradient Descent")), color = "black", size = 3, shape = 4, alpha = 0.55, width = 8, height = 8) +
      geom_jitter(data = subset(data, name %in% c("Gradient Descent")), color = "black", size = 3, shape = 4, alpha = 0.55, width = 8, height = 8) +
      geom_jitter(data = subset(data, name %in% c("Gradient Descent")), color = "black", size = 3, shape = 4, alpha = 0.45, width = 8, height = 8) +
      geom_jitter(data = subset(data, name %in% c("Gradient Descent")), color = "black", size = 3, shape = 4, alpha = 0.45, width = 8, height = 8) +
      scale_fill_distiller(palette = "Greys", direction = -1, limits = c(min(objective_data$Y) - 1000, max(objective_data$Y))) +
      geom_point(data = subset(data, min == TRUE), color = "red", shape = 3, size = 9, alpha = 1, stroke = 2) +
      theme_bw(base_size = 30) +
      theme(panel.grid = element_blank(),
            text = element_text(family="Noto Serif"),
            strip.background = element_rect(fill = "white"),
            axis.text.x = element_blank(),
            axis.ticks.x = element_blank(),
            axis.text.y = element_blank(),
            axis.ticks.y = element_blank())
#+END_SRC

#+RESULTS:
[[file:./img/sampling_comparison.pdf]]
** Sampling & D-Optimal Designs
*** Generate Data
#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(ggplot2)
library(DoE.wrapper)
library(AlgDesign)
library(dplyr)

sample_size <- 10
pre_sample_size <- 2 * sample_size
search_space_size <- 100

objective_data <- expand.grid(seq(0, 100, 1),
                              seq(0, 100, 1))
names(objective_data) <- c("x1", "x2")
objective_data$Y <- ((objective_data$x1 - (search_space_size / 2)) ^ 2) + ((objective_data$x2 - (search_space_size / 2)) ^ 2)

full_factorial <- gen.factorial(c(search_space_size, search_space_size), center = FALSE)
names(full_factorial) <- c("x1", "x2")
output <- optFederov(~., full_factorial, nTrials = sample_size)
dopt_data <- output$design
dopt_data$name <- rep("DOpt. Linear Model", nrow(dopt_data))
data <- dopt_data

full_factorial <- gen.factorial(c(search_space_size, search_space_size), center = FALSE)
names(full_factorial) <- c("x1", "x2")
output <- optFederov(~ . + I(x1 ^ 2) + I(x2 ^ 2), full_factorial, nTrials = sample_size)
#output <- optFederov(~ . + quad(.), full_factorial, nTrials = sample_size)
doptq_data <- output$design
doptq_data$name <- rep("DOpt. Quadratic Model", nrow(doptq_data))
data <- bind_rows(data, doptq_data)

drs_data <- data.frame(x1 = sample(0:search_space_size, pre_sample_size, replace = T),
                       x2 = sample(0:search_space_size, pre_sample_size, replace = T))
output <- optFederov(~., drs_data, nTrials = sample_size)
dopt_data <- output$design
dopt_data$name <- rep("Small RS + DOpt. Linear", nrow(dopt_data))
data <- bind_rows(data, dopt_data)

drs_data <- data.frame(x1 = sample(0:search_space_size, pre_sample_size, replace = T),
                       x2 = sample(0:search_space_size, pre_sample_size, replace = T))
output <- optFederov(~ . + I(x1 ^ 2) + I(x2 ^ 2), drs_data, nTrials = sample_size)
doptq_data <- output$design
doptq_data$name <- rep("Small RS + DOpt. Quadratic", nrow(doptq_data))
data <- bind_rows(data, doptq_data)
#+END_SRC

#+RESULTS:

*** Plot
#+HEADER: :results graphics output :session *R*
#+HEADER: :file ./img/dopt_comparison.pdf :exports none :width 11 :height 12 :eval no-export
#+BEGIN_SRC R
  library(extrafont)
  data$facet <- factor(data$name, levels = c("DOpt. Linear Model", "DOpt. Quadratic Model", "Small RS + DOpt. Linear", "Small RS + DOpt. Quadratic"))
  ggplot(data, aes(x = x1, y = x2)) +
      scale_x_continuous(limits = c(-1, 101), expand = c(0, 0)) +
      scale_y_continuous(limits = c(-1, 101), expand = c(0, 0)) +
      xlab("x") +
      ylab("y") +
      facet_wrap(facet ~ ., ncol = 2) +
      #geom_raster(data = objective_data, aes(fill = Y), show.legend = FALSE) +
      #geom_contour(data = objective_data, aes(z = Y), colour = "white", linetype = 8) + #, breaks = 1 * (2 ^ (2:20))) +
      geom_contour(data = objective_data, aes(z = Y), linetype = 1, colour = "black", alpha = 0.5, show.legend = FALSE, breaks = 1 * (2 ^ (4:20))) +
      geom_point(shape = 19, size = 2, colour = "black", alpha = 0.6) +
      scale_fill_distiller(palette = "Greys", direction = -1, limits = c(min(objective_data$Y) - 1000, max(objective_data$Y))) +
      theme_bw(base_size = 33) +
      theme(panel.grid = element_blank(),
            text = element_text(family="Noto Serif"),
            strip.background = element_rect(fill = "white"),
            axis.text.x = element_blank(),
            axis.ticks.x = element_blank(),
            axis.text.y = element_blank(),
            axis.ticks.y = element_blank())
#+END_SRC

#+RESULTS:
[[file:./img/dopt_comparison.pdf]]

* Introduction
Optimizing code for objectives such as performance and power consumption is
fundamental to the success and cost effectiveness of industrial and scientific
endeavors in High Performance Computing. A considerable amount of highly
specialized time and effort is spent in porting and optimizing code for GPUs,
FPGAs and other hardware accelerators. Experts are also needed to leverage
bleeding edge software improvements in compilers, languages, libraries and
frameworks. The automatic configuration and optimization of High Performance
Computing applications, or /autotuning/, is a technique effective in decreasing
the cost and time needed to adopt efficient hardware and software. Typical
targets for autotuning include algorithm selection, source-to-source
transformations and compiler configuration.

Autotuning can be studied as a search problem, where the objective is to
minimize single or multiple software of hardware metrics. The exploration of the
search spaces defined by configurations and optimizations present interesting
challenges to search strategies. These search spaces grow exponentially with the
number of considered configuration parameters and their possible values. They
are also difficult to extensively explore due to the often prohibitive costs of
hardware utilization and program compilation and execution times. Developing
autotuning strategies capable of producing good optimizations while minimizing
resource utilization is therefore essential. The capability of acquiring
knowledge about an optimization problem is also a desired feature of an
autotuning strategy, since this knowledge can decrease the cost of subsequent
optimizations of the same application or for the same hardware.

It is common and usually effective to use search meta-heuristics such as genetic
algorithms and simulated annealing in autotuning. These strategies usually
attempt to exploit local properties and are not capable of fully exploiting
global search space structures. They are also not much more effective in
comparison with a naive uniform random sample of the search
space\nbsp{}\cite{seymour2008comparison,knijnenburg2003combined}, and usually rely on a
large number of measurements and frequent restarts to achieve good performance
improvements. Search strategies based on gradient descent also are commonly used
in autotuning and rely on a large number of measurements. Their effectiveness
diminishes additionally in search spaces with complex local structures.
Completely automated machine learning autotuning strategies are effective in
building models for predicting important optimization parameters, but still rely
on a sizable data set for training. Large data sets are fundamental to
strategies based on machine learning since they select models from a generally
very large class.

Search strategies based on meta-heuristics, gradient descent and machine
learning require a large number of measurements to be effective, and are usually
incapable of providing knowledge about search spaces to users. At the end of
each autotuning session it is difficult to decide if and where further
exploration is warranted, and impossible to know which parameters are
responsible for the observed improvements. After exploring a search space, it is
impossible to confidently deduce its global properties since its was explored
with unknown biases.

In this paper we propose an autotuning strategy that leverages existing expert
and approximate knowledge about a problem in the form of a performance model,
and refines this initial model iteratively using empirical performance
evaluations, statistical analysis and user input. Our strategy puts a heavy
weight on decreasing the costs of autotuning by using efficient /Design of
Experiments/ strategies to minimize the number of experiments needed to find good
optimizations. Each optimization iteration uses /Analysis of Variance/ (ANOVA)
to help identify the relative significance of each configurable parameter to the
performance observations. An architecture- and problem-specific performance
model is built iteratively and with user input, enabling informed decisions on
which regions of the search space are worth exploring.

We present the performance of our approach on a Laplacian Kernel for GPUs where
the search space, global optimum and performance model approximation are known.
The experimental budget on this application were tightly constrained. The
speedups achieved and the budget utilization of our approach on this setting
motivated a more comprehensive performance evaluation. We chose the /Search
Problems in Automatic Performance Tuning/
(SPAPT)\nbsp{}\cite{balaprakash2012spapt} benchmark for this evaluation, where
our approach was able to find speedups of over 50$\times$ for some SPAPT
applications, finding speedups better than random sampling in some scenarios.
Despite using generic performance models for every SPAPT application, our
approach was able to significantly decrease the budget used to find performance
improvements.

The rest of this paper is organized as follows. Section [[Background]] presents
related work on source-to-source transformation, which is the main optimization
target in SPAPT problems, on autotuning systems and on search space exploration
strategies. Section [[Autotuning with Design of Experiments]] presents a
detailed description of the implementation of our approach and its background.
It discusses the Design of Experiments concepts we incorporate, and the ANOVA
and linear regression algorithms we use in analysis steps. Section [[Performance
Evaluation]] presents our results with the GPU Laplacian Kernel and the SPAPT
benchmark. Section [[Conclusion]] discusses our conclusions and future work.
* Background
** Source-to-source Transformation
** Autotuning
John Rice's Algorithm Selection framework\nbsp{}\cite{rice1976algorithm} is the
precursor of autotuners in various problem domains. In 1997, the PHiPAC
system\nbsp{}\cite{bilmes1997optimizing} used code generators and search scripts
to automatically generate high performance code for matrix multiplication. Since
then, systems approached different domains with a variety of strategies.
Dongarra /et al./\nbsp{}\cite{dongarra1998automatically} introduced the ATLAS
project, that optimizes dense matrix multiplication routines. The
OSKI\nbsp{}\cite{vuduc2005oski} library provides automatically tuned kernels for
sparse matrices. The FFTW\nbsp{}\cite{frigo1998fftw} library provides tuned C
subroutines for computing the Discrete Fourier Transform.
Periscope\nbsp{}\cite{gerndt2010automatic} is a distributed online autotuner for
parallel systems and single-node performance. In an effort to provide a common
representation of multiple parallel programming models, the INSIEME compiler
project\nbsp{}\cite{jordan2012multi} implements abstractions for OpenMP, MPI and
OpenCL, and generates optimized parallel code for heterogeneous multi-core
architectures.

A different approach is to combine generic search algorithms and problem
representation data structures in a single system that enables the
implementation of autotuners for different domains. The
PetaBricks\nbsp{}\cite{ansel2009petabricks} project provides a language,
compiler and autotuner, enabling the definition and selection of multiple
algorithms for the same problem. The ParamILS
framework\nbsp{}\cite{hutter2009paramils} applies stochastic local search
algorithms to algorithm configuration and parameter tuning. The OpenTuner
framework\nbsp{}\cite{ansel2014opentuner} provides ensembles of techniques that
search the same space in parallel, while exploration is managed by an
implementation of a solver of the multi-armed bandit problem.
** Search Space Exploration Strategies
#+BEGIN_CENTER
#+CAPTION: Exploration of the search space defined by $x^2 + y^2$, using a fixed budget of 50 points
#+ATTR_LATEX: :width .95\columnwidth
[[./img/sampling_comparison.pdf]]
#+END_CENTER
* Design of Experiments
An /experimental design/ determines a selection of experiments whose objective
is to identify the relationships between /factors/ and /responses/. While
factors and responses can refer to different concrete entities in other domains,
in computer experiments factors can be configuration parameters for algorithms
and compilers, for example, and responses can be the execution time or memory
consumption of a program. Each possible value of a factor is called a /level/.
The /effect/ of a factor on the measured response, without its /interactions/
with other factors, is the /main effect/ of that factor. Experimental designs
are constructed with objectives such as identifying the main effects and
building an analytical model for the response.

In this Section we use an example of /Screening/, an efficient but limited
technique for identifying main effects, to present the assumptions of a
traditional Design of Experiments methodology. We also discuss some techniques
for the construction of efficient designs for factors with different numbers and
types of levels, and present /D-Optimal/ designs, the technique we used in the
approach presented in this paper.
** Screening & Plackett-Burman Designs
Screening designs are used to identify the main effects of 2-level factors in
the initial stages of studying a problem. Interactions are not considered at
this stage, and screening designs are usually small. Identifying main effects
early enables focusing on a smaller set of factors on subsequent more detailed
experiments. A specially efficient design construction technique for screening
designs was presented by Plackett and Burman\nbsp{}\cite{plackett1946design}
in 1946. Despite having strong restrictions on the number of factors,
Plackett-Burman designs enable the identification of main effects of $n$ factors
with $n + 1$ experiments.

#+BEGIN_EXPORT latex
\begin{figure}[htpb]
{\normalsize
\begin{align*}
\mathbf{Y} = \bm{\beta}\mathbf{X} + \epsilon
\end{align*}
}
\caption{Linear model assumed in main-effect analysis of screening designs}
\label{fig:linear_assumption}
\end{figure}
#+END_EXPORT

Assuming a linear relationship between factors and the response is fundamental
for the analysis of variance using a Plackett-Burman design. For the following
example, consider the linear relationship presented in Figure
\ref{fig:linear_assumption}, where $\epsilon$ is the error term, $\mathbf{Y}$ is
the observed response, $\mathbf{X} = \left(1, x_1,\dots,x_n\right)$ is the set
of $n$ 2-level factors, and $\bm{\beta} = \left(\beta_0,\dots,\beta_n\right)$ is
the set with the /intercept/ $\beta_0$ and the corresponding /model
coefficients/.

We now present an example to illustrate the screening methodology. Suppose we
wish to minimize a performance metric $Y$ of a problem with factors
$x_1,\dots,x_8$ assuming values in $[-1, -0.8, -0.6, \dots, 0.6, 0.8, 1]$. Each
$y_i \in Y$ is computed using the formula described in Figure
\ref{fig:real_model}, but suppose that, for the purpose of this example, they
are computed by a very expensive black-box procedure. To efficiently study this
problem we decide to construct a Plackett-Burman design, which minimizes the
experiments needed to identify relevant factors. The analysis of this design
will enable us to decrease the dimension of the problem.

#+BEGIN_EXPORT latex
\begin{figure}[htpb]
{\normalsize
\begin{align*}
y_i =
\bordermatrix{~ & \bm{\beta}^{\top} \cr & 0 \cr & -1.5 \cr & 1.3 \cr & 3.1 \cr & -1.4 \cr & 1.35 \cr & 1.6} \ \ \ \cdot
\bordermatrix{~ & \mathbf{X}_i \cr & 1 \cr & x_1 \cr & x_3 \cr & x_5
\cr & x_7 \cr & x_8^{2} \cr & x_1x_3} \ \ +
\ \ \epsilon
\end{align*}
}
\caption{Real model used to obtain the data on Table \ref{tab:plackett}}
\label{fig:real_model}
\end{figure}
#+END_EXPORT

Table \ref{tab:plackett} presents the Plackett-Burman design we generated for
our problem. In this design we have the 8 2-level factors $x_1,\dots,x_8$, and
the observed response $\mathbf{Y}$. As is common when constructing screening
designs, we had to add 3 ``dummy'' factors $d_1,\dots,d_3$ to complete the 12
columns needed to construct a Plackett-Burman design for 8 factors.

#+HEADER: :results output latex :session *R* :exports results :eval no-export
#+BEGIN_SRC R
library(FrF2)
library(xtable)

set.seed(3138989)

get_cost <- function(data) {
    return((-1.5 * as.numeric(data$x1)) + (1.3 * as.numeric(data$x3)) +
           (1.6 * as.numeric(data$x1) * as.numeric(data$x3)) +
           (1.35 * as.numeric(data$x8) * as.numeric(data$x8)) +
           (3.1 * as.numeric(data$x5)) + (-1.4 * as.numeric(data$x7)) +
           rnorm(nrow(data), sd = 0.6))
}

objective_data <- expand.grid(seq(-1, 1, 0.2),
                              seq(-1, 1, 0.2),
                              seq(-1, 1, 0.2),
                              seq(-1, 1, 0.2),
                              seq(-1, 1, 0.2))

names(objective_data) <- c("x1", "x3", "x5",
                           "x7", "x8")

objective_data$Y <- get_cost(objective_data)

options(warn = -1)
design <- pb(12, factor.names = c("x1", "x2", "x3",
                                  "x4", "x5", "x6",
                                  "x7", "x8", "d1",
                                  "d2", "d3"))
options(warn = 0)

design$Y <- get_cost(design)

names(design) <- c("$x_1$", "$x_2$", "$x_3$",
                   "$x_4$", "$x_5$", "$x_6$",
                   "$x_7$", "$x_8$", "$d_1$",
                   "$d_2$", "$d_3$", "$Y$")

cap <- "Randomized Plackett-Burman design for factors $x_1, \\dots, x_8$, using 12 experiments and ``dummy'' factors $d_1, \\dots, d_3$, and computed response $\\mathbf{Y}$"
tab <- xtable(design, caption = cap, label = "tab:plackett")
align(tab) <- "ccccccccccccc"
print(tab, booktabs = TRUE,
      include.rownames = FALSE,
      caption.placement = "top",
      size = "\\scriptsize",
      sanitize.text.function = function(x){x})
#+END_SRC

#+RESULTS:
#+BEGIN_EXPORT latex
% latex table generated in R 3.5.1 by xtable 1.8-2 package
% Mon Oct  8 22:52:22 2018
\begin{table}[ht]
\centering
\caption{Randomized Plackett-Burman design for factors $x_1, \dots, x_8$, using 12 experiments and ``dummy'' factors $d_1, \dots, d_3$, and computed response $\mathbf{Y}$}
\label{tab:plackett}
\begingroup\scriptsize
\begin{tabular}{cccccccccccc}
  \toprule
$x_1$ & $x_2$ & $x_3$ & $x_4$ & $x_5$ & $x_6$ & $x_7$ & $x_8$ & $d_1$ & $d_2$ & $d_3$ & $Y$ \\
  \midrule
1 & -1 & 1 & 1 & 1 & -1 & -1 & -1 & 1 & -1 & 1 & 13.74 \\
  -1 & 1 & -1 & 1 & 1 & -1 & 1 & 1 & 1 & -1 & -1 & 10.19 \\
  -1 & 1 & 1 & -1 & 1 & 1 & 1 & -1 & -1 & -1 & 1 & 9.22 \\
  1 & 1 & -1 & 1 & 1 & 1 & -1 & -1 & -1 & 1 & -1 & 7.64 \\
  1 & 1 & 1 & -1 & -1 & -1 & 1 & -1 & 1 & 1 & -1 & 8.63 \\
  -1 & 1 & 1 & 1 & -1 & -1 & -1 & 1 & -1 & 1 & 1 & 11.53 \\
  -1 & -1 & -1 & 1 & -1 & 1 & 1 & -1 & 1 & 1 & 1 & 2.09 \\
  1 & 1 & -1 & -1 & -1 & 1 & -1 & 1 & 1 & -1 & 1 & 9.02 \\
  1 & -1 & -1 & -1 & 1 & -1 & 1 & 1 & -1 & 1 & 1 & 10.68 \\
  1 & -1 & 1 & 1 & -1 & 1 & 1 & 1 & -1 & -1 & -1 & 11.23 \\
  -1 & -1 & -1 & -1 & -1 & -1 & -1 & -1 & -1 & -1 & -1 & 5.33 \\
  -1 & -1 & 1 & -1 & 1 & 1 & -1 & 1 & 1 & 1 & -1 & 14.79 \\
   \bottomrule
\end{tabular}
\endgroup
\end{table}
#+END_EXPORT

We use our initial assumption shown in Figure \ref{fig:linear_assumption} to
identify the most relevant factors by performing an ANOVA test. The resulting
ANOVA table is shown in Table \ref{tab:anova_linear}, where the /significance/
of each factor can be interpreted from the F-test and P$(>\text{F})$ values.
Table \ref{tab:anova_linear} uses ``$*$'', as is convention in the \texttt{R}
language, to represent the significance values for each factor.

We see on Table \ref{tab:anova_linear} that factors $(x_3,x_5,x_7,x_8)$ have at
least one ``$*$'' of significance. For the purpose of this example, this is
reason enough to include them in our linear model for the next step. We see that
factor $x_1$ has a significance mark of ``\cdot'', but comparing its F-test and
P$(>\text{F})$ values we decide that they are fairly smaller than the values of
factors that had no significance at all, and we keep this factor. Then, since we
want to reduce the dimension of the problem, we decide in this example to not
include ($x_2,x_4,x_6)$ in our model due to their low significance.

#+HEADER: :results graphics output :session *R* :exports none :eval no-export
#+HEADER: :file ./img/main_effects.pdf
#+HEADER: :width 12 :height 4
#+BEGIN_SRC R
library(extrafont)

names(design) <- c("x1", "x2", "x3",
                   "x4", "x5", "x6",
                   "x7", "x8", "d1",
                   "d2", "d3", "Y")

regression <- lm(Y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8, data = design)

par(family = 'serif')
MEPlot(regression, main = NULL, pch = 19,
       lwd = 0, cex.xax = 2.9, cex.main = 3.1,
       cex.axis = 1)
#+END_SRC

#+RESULTS:
[[file:./img/main_effects.pdf]]

#+HEADER: :results output latex :session *R* :exports results :eval no-export
#+BEGIN_SRC R
library(xtable)

options(warn = -1)
names(design) <- c("x1", "x2", "x3",
                   "x4", "x5", "x6",
                   "x7", "x8", "d1",
                   "d2", "d3", "Y")

regression <- aov(Y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8, data = design)
s_regression <- as.data.frame(summary.aov(regression)[[1]])
s_regression <- s_regression[1:8, c("F value", "Pr(>F)")]

s_regression$stars <- symnum(s_regression[ , "Pr(>F)"], na = FALSE,
                             cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1),
                             symbols = c("$***$", "$**$", "$*$", "$\\cdot$", " "))

names(s_regression) <- c("F value", "Pr$(<\\text{F})$", "Significance")

rownames(s_regression) <- c("$x_1$", "$x_2$", "$x_3$",
                            "$x_4$", "$x_5$", "$x_6$",
                            "$x_7$", "$x_8$")

cap <- "Shortened ANOVA table for the fit of the naive model, with significance intervals from the \\texttt{R} language"
x <- xtable(s_regression, caption = cap, digits = 3, display = c("s", "f", "f", "s"), label = "tab:anova_linear")
align(x) <- xalign(x)
options(warn = 0)
print(x, size = "\\small",
      math.style.exponents = TRUE,
      booktabs = TRUE,
      sanitize.text.function = function(x){x},
      caption.placement = "top")
#+END_SRC

#+RESULTS:
#+BEGIN_EXPORT latex
% latex table generated in R 3.5.1 by xtable 1.8-2 package
% Mon Oct  8 22:52:26 2018
\begin{table}[ht]
\centering
\caption{Shortened ANOVA table for the fit of the naive model, with significance intervals from the \texttt{R} language}
\label{tab:anova_linear}
\begingroup\small
\begin{tabular}{lrrl}
  \toprule
 & F value & Pr$(<\text{F})$ & Significance \\
  \midrule
$x_1$ & 8.382 & 0.063 & $\cdot$ \\
  $x_2$ & 0.370 & 0.586 &   \\
  $x_3$ & 80.902 & 0.003 & $**$ \\
  $x_4$ & 0.215 & 0.675 &   \\
  $x_5$ & 46.848 & 0.006 & $**$ \\
  $x_6$ & 5.154 & 0.108 &   \\
  $x_7$ & 13.831 & 0.034 & $*$ \\
  $x_8$ & 59.768 & 0.004 & $**$ \\
   \bottomrule
\end{tabular}
\endgroup
\end{table}
#+END_EXPORT

Moving forward, we will build a linear model using the $(x_1,x_3,x_5,x_7,x_8)$
factors, fit the model using the values of $Y$ we obtained when running our
design, and use the coefficients of this fitted model to predict the levels for
each factor that minimize the real response. The prediction step will be run
using a full factorial combination of the possible values of
$(x_1,x_3,x_5,x_7,x_8$), without running any new experiments. The levels of the
selected factors with the best prediction on this data will be the output of
this step.

Table \ref{tab:linear_prediction_comparison} compares the prediction for $Y$
from our linear model with the selected factors $(x_1,x_3,x_5,x_7,x_8)$ with the
actual global minimum $Y$ for this problem. Using 12 measurements and a simple
linear model, the predicted best value of $Y$ was around $10\times$ larger than
the global optimum. Note that the model predicted the correct levels for $x_3$
and $x_5$, and almost predicted correctly for $x_7$. The linear model predicted
wrong levels for $x_1$, perhaps due to this factor's interaction with $x_3$, and
for $x_8$. Arguably, it would be impossible to predict the correct level for
$x_8$ using this linear model, since we know a quadratic term composes the
formula of $Y$.

#+HEADER: :results output latex :session *R* :exports results :eval no-export
#+BEGIN_SRC R
library(xtable)
library(dplyr)

names(design) <- c("x1", "x2", "x3",
                   "x4", "x5", "x6",
                   "x7", "x8", "d1",
                   "d2", "d3", "Y")

design <- lapply(design, function(x){return(as.numeric(as.character(x)))})

regression <- lm(Y ~ x1 + x3 + x5 + x7 + x8, data = design)
prediction <- predict(regression, newdata = objective_data)

comparison_data <- objective_data[prediction == min(prediction), ]
comparison_data <- bind_rows(comparison_data, objective_data[objective_data$Y == min(objective_data$Y), ])
rownames(comparison_data) <- c("Linear Model", "Global Minimum")

names(comparison_data) <- c("$x_1$", "$x_3$", "$x_5$",
                            "$x_7$", "$x_8$", "$Y$")

cap <- "Comparison of the response $Y$ predicted by the linear model and the global minimum $Y$"
x <- xtable(comparison_data, caption = cap, digits = c(1, 1, 1, 1, 1, 1, 3), label = "tab:linear_prediction_comparison")
align(x) <- xalign(x)
options(warn = 0)
print(x,
      size = "\\footnotesize",
      math.style.exponents = TRUE,
      booktabs = TRUE,
      include.rownames = TRUE,
      sanitize.text.function = function(x){x},
      caption.placement = "top")
#+END_SRC

#+RESULTS:
#+BEGIN_EXPORT latex
% latex table generated in R 3.5.1 by xtable 1.8-2 package
% Tue Oct  9 13:41:21 2018
\begin{table}[ht]
\centering
\caption{Comparison of the response $Y$ predicted by the linear model and the global minimum $Y$}
\label{tab:linear_prediction_comparison}
\begingroup\footnotesize
\begin{tabular}{lrrrrrr}
  \toprule
 & $x_1$ & $x_3$ & $x_5$ & $x_7$ & $x_8$ & $Y$ \\
  \midrule
Linear Model & -1.0 & -1.0 & -1.0 & 1.0 & -1.0 & -1.046 \\
  Global Minimum & 1.0 & -1.0 & -1.0 & 0.8 & 0.0 & -9.934 \\
   \bottomrule
\end{tabular}
\endgroup
\end{table}
#+END_EXPORT

We can improve upon this result if we introduce some information about the
problem and use a more flexible design construction technique. Next, we will
discuss the construction of efficient designs using problem-specific formulas
and continue the optimization of our example.
** D-Optimal Designs
The application of Design of Experiments to autotuning problems requires design
construction techniques that support factors of different types and number of
possible values. Autotuning problems typically combine factors such as binary
flags, integer and floating point numerical values, and unordered enumerations
of abstract values. Previously, to construct a Plackett-Burman design for our
example we had to restrict our factors to the extremes of their levels in the
interval $[-1, -0.8, -0.6,\dots,0.6, 0.8, 1]$, because such screening designs
only support 2-level factors. Doing that makes it impossible to measure the
significance of quadratic terms in the model, for example. Next we will continue
optimizing our example by constructing /D-Optimal designs/ using a more flexible
construction technique, that increases the number of levels we can screen for
and enables detecting the significance of more complex model terms.

The class of /D-Optimal designs/ is the best fit for our requirements of
supporting multi-level factors while minimizing the number of experiments. The
algorithms for constructing D-Optimal designs are relatively fast, simple, and
have few restrictions. To construct a D-Optimal design it is necessary to choose
an initial model, which can be done based on previous experiments or on expert
knowledge of the problem.

Once a model is selected, algorithmic construction is performed by searching for
the set of experiments that minimizes /D-Optimality/, a measure of the
/variance/ of the /estimators/ for the /regression coefficients/ associated with
the selected model. This search is usually done by swapping experiments from the
current candidate set with experiments from a pool of possible experiments,
according to certain rules, until some stopping criterion is met. In the example
in this Section, as well as in the approach presented in this paper, we use
Fedorov's algorithm\nbsp{}\cite{fedorov1972theory} for constructing D-Optimal
designs, implemented in =R= in the =AlgDesign= package.

Going back to our example, suppose that in addition to using our previous
screening results we decide to hire an expert in our problem's domain. The
expert confirms our initial assumptions that the factor $x_1$ should be included
in our model since it is usually relevant for this kind of problem and has a
strong interaction with factor $x_3$. She also mentions we should replace
the linear term for $x_8$ by a quadratic term for this factor.

Using our previous screening and the domain knowledge provided by our expert, we
choose a new performance model and use it to construct a D-Optimal design using
Fedorov's algorithm. Since we need enough degrees of freedom to fit our model,
we construct the design with 12 experiments shown in Table \ref{tab:d_optimal}.

#+HEADER: :results output latex :session *R* :exports results :eval no-export
#+BEGIN_SRC R
library(xtable)
library(dplyr)
library(AlgDesign)

output <- optFederov(~ x1 + x3 + x5 + x7 + I(x8 ^ 2) + x1:x3,
                     nTrials = 12,
                     data = objective_data)

dopt_design <- output$design

dopt_regression <- lm(Y ~ x1 + x3 + x5 + x7 + I(x8 ^ 2) + x1:x3, data = dopt_design)
dopt_prediction <- predict(dopt_regression, newdata = objective_data)

dopt_data <- objective_data[dopt_prediction == min(dopt_prediction), ]
names(dopt_data) <- c("$x_1$", "$x_3$", "$x_5$",
                      "$x_7$", "$x_8$", "$Y$")

names(dopt_design) <- c("$x_1$", "$x_3$", "$x_5$",
                        "$x_7$", "$x_8$", "$Y$")

cap <- "D-Optimal design constructed for the factors $(x_1,x_3,x_5,x_7,x_8)$ and computed response $Y$"
x <- xtable(dopt_design, caption = cap, digits = c(1, 1, 1, 1, 1, 1, 3), label = "tab:d_optimal")
align(x) <- xalign(x)
options(warn = 0)
print(x,
      size = "\\footnotesize",
      math.style.exponents = TRUE,
      booktabs = TRUE,
      include.rownames = FALSE,
      sanitize.text.function = function(x){x},
      caption.placement = "top")
#+END_SRC

#+RESULTS:
#+BEGIN_EXPORT latex
% latex table generated in R 3.5.1 by xtable 1.8-2 package
% Tue Oct  9 13:01:01 2018
\begin{table}[ht]
\centering
\caption{D-Optimal design constructed for the factors $(x_1,x_3,x_5,x_7,x_8)$ and computed response $Y$}
\label{tab:d_optimal}
\begingroup\footnotesize
\begin{tabular}{rrrrrr}
  \toprule
$x_1$ & $x_3$ & $x_5$ & $x_7$ & $x_8$ & $Y$ \\
  \midrule
-1.0 & -1.0 & -1.0 & -1.0 & -1.0 & 2.455 \\
  1.0 & -1.0 & -1.0 & -1.0 & -1.0 & -4.881 \\
  1.0 & -1.0 & 1.0 & -1.0 & -1.0 & 2.128 \\
  -1.0 & 1.0 & -1.0 & 1.0 & -1.0 & -2.042 \\
  -1.0 & -1.0 & 1.0 & 1.0 & -1.0 & 4.609 \\
  1.0 & 1.0 & 1.0 & 1.0 & -1.0 & 4.163 \\
  1.0 & 1.0 & -1.0 & -1.0 & 0.0 & 0.862 \\
  -1.0 & -1.0 & 1.0 & -1.0 & 0.0 & 6.453 \\
  -1.0 & 1.0 & 1.0 & -1.0 & 0.0 & 5.703 \\
  -1.0 & -1.0 & -1.0 & 1.0 & 0.0 & -2.708 \\
  1.0 & -1.0 & -1.0 & 1.0 & 0.0 & -9.019 \\
  1.0 & -1.0 & 1.0 & 1.0 & 0.0 & -2.187 \\
   \bottomrule
\end{tabular}
\endgroup
\end{table}
#+END_EXPORT

#+HEADER: :results output latex :session *R* :exports none :eval no-export
#+BEGIN_SRC R
s_regression <- as.data.frame(summary.aov(dopt_regression)[[1]])
s_regression <- s_regression[1:7, c("F value", "Pr(>F)")]

rownames(s_regression) <- c("$x_1$", "$x_3$", "$x_5$",
                            "$x_7$", "$x_8$", "I($x_8^2$)",
                            "$x_1$:$x_3$")

s_regression$stars <- symnum(s_regression[ , "Pr(>F)"], na = FALSE,
                             cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1),
                             symbols = c("$***$", "$**$", "$*$", "$\\cdot$", " "))

names(s_regression) <- c("F value", "Pr$(<\\text{F})$", "Significance")

cap <- paste("Shortened ANOVA table for the fit of the naive model (", attributes(s_regression$Significance)$legend, ")", sep = "")
x <- xtable(s_regression, caption = cap, digits = 3, display = c("s", "f", "f", "s"))
align(x) <- xalign(x)
options(warn = 0)
print(x, size = "\\small",
      math.style.exponents = TRUE,
      booktabs = TRUE,
      sanitize.text.function = function(x){x},
      caption.placement = "top")
#+END_SRC

#+RESULTS:
#+BEGIN_EXPORT latex
% latex table generated in R 3.5.1 by xtable 1.8-2 package
% Mon Oct  8 20:44:47 2018
\begin{table}[ht]
\centering
\caption{Shortened ANOVA table for the fit of the naive model (0 ‘$***$’ 0.001 ‘$**$’ 0.01 ‘$*$’ 0.05 ‘$\cdot$’ 0.1 ‘ ’ 1)}
\begingroup\small
\begin{tabular}{lrrl}
  \toprule
 & F value & Pr$(<\text{F})$ & Significance \\
  \midrule
$x_1$ & 199.361 & 0.000 & $***$ \\
  $x_3$ & 89.678 & 0.001 & $***$ \\
  $x_5$ & 297.111 & 0.000 & $***$ \\
  $x_7$ & 168.227 & 0.000 & $***$ \\
  $x_8$ & 1.916 & 0.238 &   \\
  I($x_8^2$) & 6.478 & 0.064 & $\cdot$ \\
  $x_1$:$x_3$ & 97.079 & 0.001 & $***$ \\
   \bottomrule
\end{tabular}
\endgroup
\end{table}
#+END_EXPORT

Our current performance model was constructed by the screening experiment we ran
on the previous step, and domain knowledge provided by our hired expert. We are
now going to fit this model using the results of the experiments in our
D-Optimal design. Table \ref{tab:correct_fit} shows the model fit table and
compares the estimated and real model coefficients. This example illustrates
that the Design of Experiments approach can achieve close model estimations
using few resources, provided are able to use user input to identify relevant
factors and knowledge about the problem domain to tweak the model.

#+HEADER: :results output latex :session *R* :exports results :eval no-export
#+BEGIN_SRC R
s_regression <- as.data.frame(coef(summary.lm(dopt_regression)))
s_regression <- s_regression[, c("Estimate", "t value", "Pr(>|t|)")]

rownames(s_regression) <- c("Intercept", "$x_1$", "$x_3$", "$x_5$",
                            "$x_7$", "I($x_8^2$)","$x_1$:$x_3$")

s_regression$Significance <- symnum(s_regression[ , "Pr(>|t|)"], na = FALSE,
                                    cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1),
                                    symbols = c("***", "**", "*", ".", " "))

names(s_regression) <- c("Estimated", "t value", "Pr$(>|\\text{t}|)$", "Signif.")
s_regression$Real <- c(0, -1.5, 1.3, 3.1, -1.4, 1.35, 1.6)

s_regression <- s_regression[ , c("Real", "Estimated", "t value", "Pr$(>|\\text{t}|)$", "Signif.")]

cap <- "Correct model fit comparing real and estimated coefficients, with significance intervals from the \\texttt{R} language"
x <- xtable(s_regression, caption = cap, digits = 3, display = c("s", "f", "f", "f", "f", "s"), label = "tab:correct_fit")
align(x) <- xalign(x)
options(warn = 0)
print(x, size = "\\small",
      math.style.exponents = TRUE,
      booktabs = TRUE,
      sanitize.text.function = function(x){x},
      caption.placement = "top")
#+END_SRC

#+RESULTS:
#+BEGIN_EXPORT latex
% latex table generated in R 3.5.1 by xtable 1.8-2 package
% Tue Oct  9 13:20:13 2018
\begin{table}[ht]
\centering
\caption{Correct model fit comparing real and estimated coefficients, with significance intervals from the \texttt{R} language}
\label{tab:correct_fit}
\begingroup\small
\begin{tabular}{lrrrrl}
  \toprule
 & Real & Estimated & t value & Pr$(>|\text{t}|)$ & Signif. \\
  \midrule
Intercept & 0.000 & 0.278 & 1.192 & 0.287 &   \\
  $x_1$ & -1.500 & -1.378 & -8.116 & 0.000 & *** \\
  $x_3$ & 1.300 & 1.283 & 7.558 & 0.001 & *** \\
  $x_5$ & 3.100 & 3.017 & 18.851 & 0.000 & *** \\
  $x_7$ & -1.400 & -1.659 & -10.365 & 0.000 & *** \\
  I($x_8^2$) & 1.350 & 1.222 & 3.816 & 0.012 & * \\
  $x_1$:$x_3$ & 1.600 & 1.718 & 10.124 & 0.000 & *** \\
   \bottomrule
\end{tabular}
\endgroup
\end{table}
#+END_EXPORT

Table \ref{tab:prediction_comparisons} compares the global minimum in this
example with the predictions made by our initial linear model from the screening
step and our improved model from this step. Using screening, D-Optimal designs,
and domain knowledge we found an optimization within $10\%$ of the global
optimum computing $Y$ only 24 times. We were able to do that by first reducing
the dimension of the problem when we eliminated irrelevant factors in the
screening step. We then constructed a more careful exploration of this new
problem subspace, helped by domain knowledge provided by an expert.

#+HEADER: :results output latex :session *R* :exports results :eval no-export
#+BEGIN_SRC R
dopt_comparison_data <- bind_rows(dopt_data, comparison_data)
rownames(dopt_comparison_data) <- c("Correct Model", "Linear Model", "Global Minimum")

cap <- "Comparison of the response $Y$ predicted by our models and the global minimum $Y$"
x <- xtable(dopt_comparison_data, caption = cap, digits = c(2, 2, 2, 2, 2, 2, 3), label = "tab:prediction_comparisons")
align(x) <- xalign(x)
options(warn = 0)
print(x,
      size = "\\footnotesize",
      math.style.exponents = TRUE,
      booktabs = TRUE,
      include.rownames = TRUE,
      sanitize.text.function = function(x){x},
      caption.placement = "top")
#+END_SRC

#+RESULTS:
#+BEGIN_EXPORT latex
% latex table generated in R 3.5.1 by xtable 1.8-2 package
% Tue Oct  9 13:41:52 2018
\begin{table}[ht]
\centering
\caption{Comparison of the response $Y$ predicted by our models and the global minimum $Y$}
\label{tab:prediction_comparisons}
\begingroup\footnotesize
\begin{tabular}{lrrrrrr}
  \toprule
 & $x_1$ & $x_3$ & $x_5$ & $x_7$ & $x_8$ & $Y$ \\
  \midrule
Correct Model & 1.00 & -1.00 & -1.00 & 1.00 & 0.00 & -9.019 \\
  Linear Model & -1.00 & -1.00 & -1.00 & 1.00 & -1.00 & -1.046 \\
  Global Minimum & 1.00 & -1.00 & -1.00 & 0.80 & 0.00 & -9.934 \\
   \bottomrule
\end{tabular}
\endgroup
\end{table}
#+END_EXPORT

We are able to explain the performance improvements we obtained in each step of
the process, because we finish steps with a performance model and a performance
prediction. Each factor is included or removed using information obtained in
statistical tests or expert knowledge. If we need to optimize this problem
again, for a different architecture or with larger input, for example, we are
able to start exploring the search space with a less naive model.

The process of screening for factor significance using ANOVA and fitting a
new model using acquired knowledge is essentially a step in the transparent
Design of Experiments approach we present in the next Section.
*** Optimizing the Rosenbrock Function                           :noexport:
#+CAPTION: Defining the Rosenbrock function in =R=
#+BEGIN_figure
#+HEADER: :results output :session *R* :exports code :eval no-export
#+BEGIN_SRC R
rosenbrock <- function(x, y) {
    return(((1.0 - x) ^ 2) + (100.0 * ((y - (x ^ 2)) ^ 2)))
}
#+END_SRC

#+RESULTS:

#+END_FIGURE

#+HEADER: :results output latex :session *R* :exports results :eval no-export
#+BEGIN_SRC R
library(xtable)

cap <- "Comparison of 3 optimization methods on Rosenbrock's function, using a buget of 10 points with 100 repetitions"
rosenbrock_summaries <- read.csv(file = "./data/rosenbrock_summaries.csv", header = TRUE)
x <- xtable(rosenbrock_summaries, caption = cap, display = c("s", "s", "g", "g", "g"), digits = 2)
align(x) <- xalign(x)
print(x, size = "\\small", include.rownames = FALSE, booktabs = TRUE, math.style.exponents = TRUE, caption.placement = "top")
#+END_SRC

#+RESULTS:
#+BEGIN_EXPORT latex
% latex table generated in R 3.5.1 by xtable 1.8-2 package
% Wed Oct  3 11:29:52 2018
\begin{table}[ht]
\centering
\begingroup\small
\begin{tabular}{lrrr}
  \toprule
Method & Mean & Min. & Max. \\
  \midrule
Random Sampling & $1.3 \times 10^{2}$ & 1.2 & $1.5 \times 10^{3}$ \\
  D-Opt. w/ Linear Model & $1.4 \times 10^{4}$ & $1.4 \times 10^{4}$ & $1.4 \times 10^{4}$ \\
  D-Opt. w/ Correct Model &   0 &   0 &   0 \\
   \bottomrule
\end{tabular}
\endgroup
\caption{Comparison of 3 optimization methods on Rosenbrock's function, using a buget of 10 points with 100 repetitions}
\end{table}
#+END_EXPORT

#+BEGIN_CENTER
#+CAPTION: Contour plot in $log_{10}$ scale and global optimum of Rosenbrock's function
#+BEGIN_figure
#+ATTR_LATEX: :width .8\columnwidth
[[./img/rosenbrock.pdf]]
#+END_FIGURE
#+END_CENTER

#+HEADER: :results output latex :session *R* :exports results :eval no-export
#+BEGIN_SRC R
library(AlgDesign)
library(xtable)

rosenbrock <- function(x, y) {
    return(((1.0 - x) ^ 2) + (100.0 * ((y - (x ^ 2)) ^ 2)) + rnorm(1, sd = 10))
}

rosenbrock_data <- expand.grid(seq(-4, 4, 0.05),
                               seq(-4, 4, 0.05))

names(rosenbrock_data) <- c("x", "y")
rosenbrock_data$Y <- mapply(rosenbrock,
                            rosenbrock_data$x,
                            rosenbrock_data$y)

output <- optFederov(~ x + y + I(x ^ 4) + I(y ^ 2) + I(y ^ 2) + I(x ^ 2):y, data = rosenbrock_data, nTrials = 10)
regression <- aov(Y ~ x + y + I(x ^ 4) + I(x ^ 2) + I(y ^ 2) + I(x ^ 2):y, data = output$design)
s_regression <- as.data.frame(summary.aov(regression)[[1]])
s_regression <- s_regression[1:6, c("F value", "Pr(>F)")]

cap <- "Shortened ANOVA table for the fit of the correct model using 10 experiments"
x <- xtable(s_regression, caption = cap, display = c("s","g", "g"), digits = 2)
align(x) <- xalign(x)
print(x, size = "\\small", math.style.exponents = TRUE, booktabs = TRUE, caption.placement = "top")
#+END_SRC

#+RESULTS:
#+BEGIN_EXPORT latex
% latex table generated in R 3.5.1 by xtable 1.8-2 package
% Wed Oct  3 16:57:45 2018
\begin{table}[ht]
\centering
\caption{Shortened ANOVA table for the fit of the correct model using 10 experiments}
\begingroup\small
\begin{tabular}{lrr}
  \toprule
 & F value & Pr($>$F) \\
  \midrule
x           & $2 \times 10^{4}$ & $7.5 \times 10^{-7}$ \\
  y           & $9.2 \times 10^{6}$ & $7.9 \times 10^{-11}$ \\
  I(x\verb|^|4)      & $2 \times 10^{7}$ & $2.4 \times 10^{-11}$ \\
  I(x\verb|^|2)      & $3.3 \times 10^{5}$ & $1.2 \times 10^{-8}$ \\
  I(y\verb|^|2)      & $3 \times 10^{4}$ & $4.2 \times 10^{-7}$ \\
  y:I(x\verb|^|2)    & $3.9 \times 10^{6}$ & $2.8 \times 10^{-10}$ \\
   \bottomrule
\end{tabular}
\endgroup
\end{table}
#+END_EXPORT

#+HEADER: :results output latex :session *R* :exports results :eval no-export
#+BEGIN_SRC R
library(AlgDesign)
library(xtable)

rosenbrock <- function(x, y) {
    return(((1.0 - x) ^ 2) + (100.0 * ((y - (x ^ 2)) ^ 2)))
}

rosenbrock_data <- expand.grid(seq(-4, 4, 0.05),
                               seq(-4, 4, 0.05))

names(rosenbrock_data) <- c("x", "y")
rosenbrock_data$Y <- mapply(rosenbrock,
                            rosenbrock_data$x,
                            rosenbrock_data$y)

output <- optFederov(~ ., data = rosenbrock_data, nTrials = 10)
regression <- lm(Y ~ ., data = output$design)
s_regression <- as.data.frame(summary.aov(regression)[[1]])
s_regression <- s_regression[1:2, c("F value", "Pr(>F)")]

cap <- "Shortened ANOVA table for the fit of the naive linear model using 10 experiments"
x <- xtable(s_regression, caption = cap, display = c("s","g", "g"), digits = 2)
align(x) <- xalign(x)
print(x, size = "\\small", math.style.exponents = TRUE, booktabs = TRUE, caption.placement = "top")
#+END_SRC

#+RESULTS:
#+BEGIN_EXPORT latex
% latex table generated in R 3.5.1 by xtable 1.8-2 package
% Wed Oct  3 13:38:41 2018
\begin{table}[ht]
\centering
\caption{Shortened ANOVA table for the fit of the naive linear model using 10 experiments}
\begingroup\small
\begin{tabular}{lrr}
  \toprule
 & F value & Pr($>$F) \\
  \midrule
x           & $7.5 \times 10^{-6}$ &   1 \\
  y           & 1.4 & 0.27 \\
   \bottomrule
\end{tabular}
\endgroup
\end{table}
#+END_EXPORT
* Autotuning with Design of Experiments
In this Section we discuss in detail our iterative Design of Experiments
approach to autotuning. At the start of the process it is necessary to define
the factors and levels that compose the search space of the target problem,
select an initial performance model, and generate an experimental design. Then,
as discussed in the previous Section, we identify relevant factors by running an
ANOVA test on the results. This enables selecting and fitting a new performance
model, which is used for predicting levels for each relevant factor. The process
can then restart, generating a new design for the new problem subspace. Informed
decisions made by the user play a central role in each iteration, guiding and
speeding up the process. Figure [[fig:doe_anova_strategy]] presents an overview of
our approach.

The first step of our approach is to define which are the target factors and
which levels of each factor are worth exploring. Then, the user must select an
initial performance model. Compilers typically expose many 2-level factors in
the form of configuration flags. The performance model for a single flag can
only be a linear term, since there are only 2 values to measure. Interactions
between flags can also be considered in an initial model. Numerical factors are
also common, such as block sizes for CUDA programs or loop unrolling amounts.
Deciding which levels to include for these kinds of factors requires a more
careful analysis. For example, if we suspect the performance model has a
quadratic term for a certain factor, we must include at least three of its
levels. We can always consider the entire valid range of numerical factors.
Other compiler parameters such as \texttt{-O(0,1,2,3)} have no clear ordering
between their levels. These are categorical factors, and must be treated
differently when constructing designs and analyzing the results.

#+BEGIN_CENTER
#+NAME: fig:doe_anova_strategy
#+CAPTION: Overview of the Design of Experiments approach to autotuning proposed in this paper
#+ATTR_LATEX: :width .95\columnwidth
#+ATTR_ORG: :width 400
[[./img/doe_anova_strategy.pdf]]
#+END_CENTER

We decided to use D-Optimal designs because their construction techniques enable
mixing categorical and numerical factors in the same screening design, while
biasing sampling according to a performance model. This enables the autotuner to
exploit global search space structures if we use the right model. When
constructing a D-Optimal design the user can require that specific points in the
search space are included, or that others are not. Algorithms for constructing
D-Optimal designs are capable of adapting to these requirements by optimizing a
starting design. Before settling on D-Optimal designs, we explored other design
construction techniques such as the
Plackett-Burman\nbsp{}\cite{plackett1946design} screening designs shown in the
previous Section, the /contractive replacement/ technique of
Addelman-Kempthorne\nbsp{}\cite{addelman1961some} and the /direct generation/
algorithm by Grömping and Fontana\nbsp{}\cite{ulrike2018algorithm}. These
techniques have strong requirements on design size and level mixing, so we opted
for a more flexible technique that would enable exploring a more comprehensive
class of autotuning problems.

After the design is constructed we run each selected experiment. This step can
be done in parallel since experiments are independent. Runtime failures are
common in this step due to problems such as incorrect output. The user can
decide whether to construct a new design using the successfully completed
experiments or to continue to the analysis step if enough experiments succeed.

The next four steps of an iteration, shown in Figure [[fig:doe_anova_strategy]],
were discussed in detail in the previous Section. User input is fundamental to
the success of these steps. After running the ANOVA test, the user should apply
domain knowledge to analyze the ANOVA table and determine which factors are
relevant. Certain factors might not appear relevant but the user might still
want to include them in the model to explore more of its levels, for example.
Selecting the model after the ANOVA test also benefits from domain knowledge.
The impact of the number of threads used by a parallel program on its
performance is usually modeled using a quadratic term, for example.

A central assumption of ANOVA is the /homoscedasticity/ of the response, which
can be interpreted as requiring the observed error on measurements to be
independent of factor levels and of the number of measurements. Fortunately, up
to a point, there are statistical tests and corrections for lack of
homoscedasticity, and our approach uses those before every ANOVA step.

After the model is selected and fitted, prediction results will depend on the
size of the data set available. If it is feasible to compute the fitted model on
all possible factor combinations, we can be sure that the global optimum has a
chance of being found. If the search space is too large to be generated, we have
to adapt this step and run the prediction on a sample.

The last step on an iteration is fixing factor levels to those predicted to have
best performance. The user can also decide the level of trust that will be
placed on the model and ANOVA at this step by allowing other levels. This step
performs a reduction on the dimension of the problem by eliminating factors and
decreasing the size of the search space. If we identify relevant parameters
correctly, we will have restricted further search to better regions of the
search space. In the next Section we present the performance of our approach in
scenarios that differ on search space size, availability and complexity.
* Performance Evaluation
** Example on a GPU Laplacian Kernel
#+BEGIN_EXPORT latex
\begin{figure}
{\scriptsize
\begin{align*}
\texttt{time\_per\_pixel} \sim & \; \texttt{y\_component\_number} + 1 / \texttt{y\_component\_number} \; + \\
& \; \texttt{vector\_length} + \texttt{lws\_y} + 1 / \texttt{lws\_y} \; + \\
& \; \texttt{load\_overlap} + \texttt{temporary\_size} \; + \\
& \; \texttt{elements\_number} + 1 / \texttt{elements\_number} \; + \\
& \; \texttt{threads\_number} + 1 /\texttt{threads\_number}
\end{align*}
}
\caption{Initial performance model used by LM and DLMT}
\end{figure}
#+END_EXPORT

#+ATTR_LATEX: :booktabs t :align ll :font \footnotesize :float t :placement [ht]
#+CAPTION: Algorithms compared in the GPU Laplacian Kernel
|------+-----------------------------|
|      | Algorithm                   |
|------+-----------------------------|
| RS   | Random Sampling             |
| LHS  | Latin Hyper Square Sampling |
| GS   | Greedy Search               |
| GSR  | Greedy Search w/ Restart    |
| GA   | Genetic Algorithm           |
| LM   | Iterative Linear Model      |
| DLMT | D-Optimal Designs           |
|------+-----------------------------|

*** Results
#+HEADER: :results output latex :session *R* :exports results :eval no-export
#+BEGIN_SRC R
library(xtable)

df_all_methods <- read.csv("./data/complete_1000.csv", strip.white = T, header = T)
df_all_methods$method <- factor(df_all_methods$method, levels = c("RS","LHS","GS","GSR","GA","LM", "LMB", "LMBT", "RQ", "DOPT", "DLM", "DLMT"))
df_all_methods <- subset(df_all_methods, method %in% c("RS", "LHS", "GS", "GSR", "GA", "LM", "DLMT"))

summaries <- data.frame(RS = c(as.data.frame(unclass(summary(df_all_methods[df_all_methods$method == "RS", ]$slowdown)))[ , 1],
                              mean(df_all_methods[df_all_methods$method == "RS",]$point_number),
                              max(df_all_methods[df_all_methods$method == "RS",]$point_number)),
                        LHS = c(as.data.frame(unclass(summary(df_all_methods[df_all_methods$method == "LHS", ]$slowdown)))[ , 1],
                                mean(df_all_methods[df_all_methods$method == "LHS",]$point_number),
                                max(df_all_methods[df_all_methods$method == "LHS",]$point_number)),
                        GS = c(as.data.frame(unclass(summary(df_all_methods[df_all_methods$method == "GS", ]$slowdown)))[ , 1],
                              mean(df_all_methods[df_all_methods$method == "GS",]$point_number),
                              max(df_all_methods[df_all_methods$method == "GS",]$point_number)),
                        GSR = c(as.data.frame(unclass(summary(df_all_methods[df_all_methods$method == "GSR", ]$slowdown)))[ , 1],
                                mean(df_all_methods[df_all_methods$method == "GSR",]$point_number),
                                max(df_all_methods[df_all_methods$method == "GSR",]$point_number)),
                        GA = c(as.data.frame(unclass(summary(df_all_methods[df_all_methods$method == "GA", ]$slowdown)))[ , 1],
                              mean(df_all_methods[df_all_methods$method == "GA",]$point_number),
                              max(df_all_methods[df_all_methods$method == "GA",]$point_number)),
                        LM = c(as.data.frame(unclass(summary(df_all_methods[df_all_methods$method == "LM", ]$slowdown)))[ , 1],
                              mean(df_all_methods[df_all_methods$method == "LM",]$point_number),
                              max(df_all_methods[df_all_methods$method == "LM",]$point_number)),
                        DLMT = c(as.data.frame(unclass(summary(df_all_methods[df_all_methods$method == "DLMT", ]$slowdown)))[ , 1],
                                    mean(df_all_methods[df_all_methods$method == "DLMT",]$point_number),
                                    max(df_all_methods[df_all_methods$method == "DLMT",]$point_number)))

rownames(summaries) <- c(rownames(as.data.frame(unclass(summary(df_all_methods[df_all_methods$method == "RS", ]$slowdown)))), "Mean Points", "Max Points")
summaries <- t(summaries)
summaries <- summaries[ , c("Mean", "Min.", "Max.", "Mean Points", "Max Points")]

cap <- "Slowdown and budget used by 7 optimization methods on the Laplacian Kernel, using a budget of 125 points with 1000 repetitions"
x <- xtable(summaries, caption = cap, digits = 2)
align(x) <- xalign(x)
display(x) <- display(x)
print(x, size = "\\small", booktabs = TRUE, math.style.exponents = TRUE, caption.placement = "top")
#+END_SRC

#+RESULTS:
#+BEGIN_EXPORT latex
% latex table generated in R 3.5.1 by xtable 1.8-2 package
% Thu Oct  4 17:04:18 2018
\begin{table}[ht]
\centering
\caption{Comparison of slowdown and budget used by 7 optimization methods on the Laplacian Kernel, using a budget of 125 points with 1000 repetitions}
\begingroup\small
\begin{tabular}{lrrrrr}
  \toprule
 & Mean & Min. & Max. & Mean Points & Max Points \\
  \midrule
RS & 1.10 & 1.00 & 1.39 & 120.00 & 120.00 \\
  LHS & 1.17 & 1.00 & 1.52 & 98.92 & 125.00 \\
  GS & 6.46 & 1.00 & 124.76 & 22.17 & 106.00 \\
  GSR & 1.23 & 1.00 & 3.16 & 120.00 & 120.00 \\
  GA & 1.12 & 1.00 & 1.65 & 120.00 & 120.00 \\
  LM & 1.02 & 1.01 & 3.77 & 119.00 & 119.00 \\
  DLMT & 1.01 & 1.01 & 1.01 & 54.84 & 56.00 \\
   \bottomrule
\end{tabular}
\endgroup
\end{table}
#+END_EXPORT

#+BEGIN_CENTER
#+CAPTION: Histograms of 7 optimization methods on the Laplacian Kernel, using a budget of 125 points with 1000 repetitions
#+ATTR_LATEX: :float t :placement [ht] :width .9\columnwidth
[[./img/comparison_histogram.pdf]]
#+END_CENTER
** Results on the SPAPT Benchmark
*** The SPAPT Benchmark
#+ATTR_LATEX: :booktabs t :align llll :font \scriptsize :float t :placement [ht]
#+CAPTION: Set of applications we used from the SPAPT benchmark
|---------------+---------------------------------+---------+----------------------|
| Kernel        | Operation                       | Factors | Size                 |
|---------------+---------------------------------+---------+----------------------|
| =atax=        | Matrix transp. & vector mult.   |      18 | $2.6 \times 10^{16}$ |
| =dgemv3=      | Scalar, vector & matrix mult.   |      49 | $3.8 \times 10^{36}$ |
| =gemver=      | Vector mult. & matrix add.      |      24 | $2.6 \times 10^{22}$ |
| =gesummv=     | Scalar, vector, & matrix mult.  |      11 | $5.3 \times 10^{9}$  |
| =hessian=     | Hessian computation             |       9 | $3.7 \times 10^{7}$  |
| =mm=          | Matrix multiplication           |      13 | $1.2 \times 10^{12}$ |
| =mvt=         | Matrix vector product & transp. |      12 | $1.1 \times 10^{9}$  |
| =tensor=      | Tensor matrix mult.             |      20 | $1.2 \times 10^{19}$ |
| =trmm=        | Triangular matrix operations    |      25 | $3.7 \times 10^{23}$ |
| =bicg=        | Subkernel of BiCGStab           |      13 | $3.2 \times 10^{11}$ |
| =lu=          | LU decomposition                |      14 | $9.6 \times 10^{12}$ |
| =adi=         | Matrix sub., mult., & div.      |      20 | $6.0 \times 10^{15}$ |
| =jacobi=      | 1-D Jacobi computation          |      11 | $5.3 \times 10^{9}$  |
| =seidel=      | Matrix factorization            |      15 | $1.3 \times 10^{14}$ |
| =stencil3d=   | 3-D stencil computation         |      29 | $9.7 \times 10^{27}$ |
| =correlation= | Correlation computation         |      21 | $4.5 \times 10^{17}$ |
|---------------+---------------------------------+---------+----------------------|
*** Experimental Methodology
**** Effect of Sampling in D-Optimal Designs

#+BEGIN_CENTER
#+CAPTION: Effect of a small initial candidate set on constructing D-Optimal designs
#+ATTR_LATEX: :width .75\columnwidth
[[./img/dopt_comparison.pdf]]
#+END_CENTER
*** Results                                                      :noexport:
#+BEGIN_CENTER
#+CAPTION: Results
#+ATTR_LATEX: :float multicolumn :placement [p] :width .9\textwidth
[[./img/iteration_best_comparison.pdf]]
#+END_CENTER
\clearpage
* Conclusion
* Acknowledgment
:PROPERTIES:
:UNNUMBERED: t
:END:
#+LATEX: \bibliographystyle{IEEEtran}
#+LATEX: \bibliography{references}
