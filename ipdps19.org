# -*- mode: org -*-
# -*- coding: utf-8 -*-
#+STARTUP: overview indent inlineimages logdrawer

#+TITLE: A Design of Experiments Approach to Autotuning under Tight Budget Constraints
#+LANGUAGE:    en
#+TAGS: noexport(n) Stats(S)
#+TAGS: Teaching(T) R(R) OrgMode(O) Python(P)
#+TAGS: Book(b) DOE(D) Code(C) NODAL(N) FPGA(F) Autotuning(A) Arnaud(r)
#+TAGS: DataVis(v) PaperReview(W)
#+EXPORT_SELECT_TAGS: Blog
#+OPTIONS:   H:3 num:t toc:nil \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t
#+OPTIONS:   TeX:t LaTeX:nil skip:nil d:nil todo:t pri:nil tags:not-in-toc
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+COLUMNS: %25ITEM %TODO %3PRIORITY %TAGS
#+SEQ_TODO: TODO(t!) STARTED(s!) WAITING(w@) APPT(a!) | DONE(d!) CANCELLED(c!) DEFERRED(f!)

#+LATEX_CLASS: org-ieeetran
#+LATEX_CLASS_OPTIONS: [conference]
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage{amssymb}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{xcolor}
#+LATEX_HEADER: \usepackage{url}
#+LATEX_HEADER: \usepackage{listings}
#+LATEX_HEADER: %\usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage[english]{babel}
#+LATEX_HEADER: \usepackage{multirow}
#+LATEX_HEADER: \usepackage{caption}
#+LATEX_HEADER: \usepackage{hyperref}
#+LATEX_HEADER: \usepackage{booktabs}
#+LATEX_HEADER: \usepackage{array}
#+LATEX_HEADER: \usepackage{relsize}
#+LATEX_HEADER: \usepackage{bm}
#+LATEX_HEADER: \usepackage{wasysym}
#+LATEX_HEADER: \usepackage{ragged2e}

#+LATEX_HEADER: \lstset{ %
#+LATEX_HEADER:   backgroundcolor={},
#+LATEX_HEADER:   basicstyle=\ttfamily\scriptsize,
#+LATEX_HEADER:   breakatwhitespace=true,
#+LATEX_HEADER:   breaklines=true,
#+LATEX_HEADER:   captionpos=n,
#+LATEX_HEADER:   extendedchars=true,
#+LATEX_HEADER:   frame=n,
#+LATEX_HEADER:   rulecolor=\color{black},
#+LATEX_HEADER:   showspaces=false,
#+LATEX_HEADER:   showstringspaces=false,
#+LATEX_HEADER:   showtabs=false,
#+LATEX_HEADER:   stepnumber=2,
#+LATEX_HEADER:   stringstyle=\color{gray},
#+LATEX_HEADER:   tabsize=2,
#+LATEX_HEADER: }
#+LATEX_HEADER: \renewcommand*{\UrlFont}{\ttfamily\smaller\relax}

#+LATEX_HEADER: \graphicspath{{./img/}}
#+LATEX_HEADER: \renewcommand*{\UrlFont}{\ttfamily\smaller\relax}

#+LATEX_HEADER: \author{\IEEEauthorblockN{Pedro Bruel\IEEEauthorrefmark{1}\IEEEauthorrefmark{2},
#+LATEX_HEADER: Arnaud Legrand\IEEEauthorrefmark{1},
#+LATEX_HEADER: Jean-Marc Vincent\IEEEauthorrefmark{1},
#+LATEX_HEADER: Brice Videau\IEEEauthorrefmark{1} and
#+LATEX_HEADER: Alfredo Goldman\IEEEauthorrefmark{2}}
#+LATEX_HEADER: \IEEEauthorblockA{\IEEEauthorrefmark{1}University of Grenoble Alpes, CNRS, INRIA, LIG - Grenoble, France\\
#+LATEX_HEADER: Email: \{arnaud.legrand, brice.videau\}@imag.fr}
#+LATEX_HEADER: \IEEEauthorblockA{\IEEEauthorrefmark{2}University of São Paulo - São Paulo, Brazil\\
#+LATEX_HEADER: Email: \{phrb, gold\}@ime.usp.br}}

#+LATEX: \begin{abstract}
Abstract
#+LATEX: \end{abstract}

* Arnaud's Draft                                                   :noexport:
** Intro
** Context
- HPC, optimizing code is a nightmare although very important gains
  can be expected when one can afford an expert to work on it.
- Typical techniques are source-to-source transformation + compiler
  flag optimization
- Even when automatic, this optimization can be very time consumming
  (costly experiments + curse of dimensionality).
** Related Work
*** Source-to-source transformation
*** Auto-tuning frameworks
*** Exploration Strategies
** Statement
- Generic Meta-Heuristics (GAs, Simulated Annealing, Tabu Search) do
  not exploit well specific properties of the problem and require very
  large amount of measurements.
- Classical Mathematical Optimization techniques (gradient, surrogate,
  ...) are ineffective in this context as the geometry is far more
  complicated than what can be found in maths textbooks
- Fully automatic ML make sense to model and predict important factors
  but typically require a large amount of data to be effective as the
  class of underlying models is generally very large.
- In many settings a naive uniform random sampling strategy works just
  as well as other methods.
- None of the above methods really brings exploitable knowledge
  allowing to decide whether further exploration may be useful.
** Proposal
Sequential approach, using D-optimal designs. Requires a model
(ideally provided by an expert) which is iteratively refined.
*** D-optimal designs in a nutshell
- Explanations of DoE + Simple illustration
- Analysis strategy (aov, lm)
- Allows a global overview and to detect the main factors right away
  to focus on the most promising parts of the subspace
- This assumes that there is a global geometry of the problem that can
  be exploited despite the roughness of the local geometry. This
  assumption may be wrong but is likely to go detected.
*** General Method in the context of auto-tuning
Ideally, human in the loop but for the sake of a general performance
evaluation, we had to automate it.
** Performance Evaluation
*** Experimental Methodology
G5K, database, RR, R + julia +...
*** Working out a simple example in details: a Laplacian Kernel
Laplacian Kernel on a GPU + BOAST
*** Evaluation on the ??? benchmark suite
ORIO
** Conclusion and Future Work
- DoE based strategy
- Revealed impressively effective for the Laplacian kernel.
- Not as impressive on the other benchmarks but despite their general
  use, it apears that little gain can be expected. In any cases, our
  approach produces at least as good results with far fewer measurements.
- Future work:
  - Other benchmarks
  - source-to-source + compiler flags
  - connexion with online learning
* Rosenbrock Example Setup                                         :noexport:
#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(AlgDesign)

rosenbrock <- function(x, y) {
    return(((1.0 - x) ^ 2) + (100.0 * ((y - (x ^ 2)) ^ 2)))
}

rosenbrock_data <- expand.grid(seq(-4, 4, 0.05), seq(-4, 4, 0.05))
rosenbrock_data$Y <- mapply(rosenbrock, rosenbrock_data$Var1, rosenbrock_data$Var2)

dim(rosenbrock_data)
rosenbrock(1, 1)
#+END_SRC

#+RESULTS:
: [1] 25921     3
: [1] 0

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
random_sample <- rosenbrock_data[sample(1:nrow(rosenbrock_data), 8, replace = TRUE), ]
dim(random_sample)
random_sample[random_sample$Y == min(random_sample$Y), ]
#+END_SRC

#+RESULTS:
: [1] 8 3
:       Var1 Var2      Y
: 16666  0.1 1.15 130.77

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
sampled_mins <- replicate(100, {
  random_sample <- rosenbrock_data[sample(1:nrow(rosenbrock_data), 10, replace = TRUE), ]
  sample_min <- random_sample[random_sample$Y == min(random_sample$Y), "Y"]
  sample_min
  })

sampled_mins <- as.numeric(unlist(sampled_mins))
random_summary <- summary(sampled_mins)
random_summary
#+END_SRC

#+RESULTS:
:     Min.  1st Qu.   Median     Mean  3rd Qu.     Max.
:    1.153   10.954   43.828  133.926  130.590 1529.620

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
i <- 0
linear_mins <- replicate(100, {
  output <- optFederov(~ ., data = rosenbrock_data, nTrials = 10)
  regression <- lm(Y ~ ., data = output$design)
  prediction <- predict(regression, rosenbrock_data)
  best_predicted <- rosenbrock_data[prediction == min(prediction), ]
  i <<- i + 1
  print(i)
  best_predicted$Y
})

linear_mins <- as.numeric(unlist(linear_mins))
linear_summary <- summary(linear_mins)
linear_summary
#+END_SRC

#+RESULTS:
#+begin_example
[1] 1
[1] 2
[1] 3
[1] 4
[1] 5
[1] 6
[1] 7
[1] 8
[1] 9
[1] 10
[1] 11
[1] 12
[1] 13
[1] 14
[1] 15
[1] 16
[1] 17
[1] 18
[1] 19
[1] 20
[1] 21
[1] 22
[1] 23
[1] 24
[1] 25
[1] 26
[1] 27
[1] 28
[1] 29
[1] 30
[1] 31
[1] 32
[1] 33
[1] 34
[1] 35
[1] 36
[1] 37
[1] 38
[1] 39
[1] 40
[1] 41
[1] 42
[1] 43
[1] 44
[1] 45
[1] 46
[1] 47
[1] 48
[1] 49
[1] 50
[1] 51
[1] 52
[1] 53
[1] 54
[1] 55
[1] 56
[1] 57
[1] 58
[1] 59
[1] 60
[1] 61
[1] 62
[1] 63
[1] 64
[1] 65
[1] 66
[1] 67
[1] 68
[1] 69
[1] 70
[1] 71
[1] 72
[1] 73
[1] 74
[1] 75
[1] 76
[1] 77
[1] 78
[1] 79
[1] 80
[1] 81
[1] 82
[1] 83
[1] 84
[1] 85
[1] 86
[1] 87
[1] 88
[1] 89
[1] 90
[1] 91
[1] 92
[1] 93
[1] 94
[1] 95
[1] 96
[1] 97
[1] 98
[1] 99
[1] 100
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
  14409   14409   14409   14416   14425   14425
#+end_example

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
i <- 0
simple_model_mins <- replicate(10, {
  output <- optFederov(~ Var1 + Var2 + I(Var1 ^ 2) + I(Var2 ^ 2), data = rosenbrock_data, nTrials = 8)
  regression <- lm(Y ~ Var1 + Var2 + I(Var1 ^ 2) + I(Var2 ^ 2), data = output$design)
  prediction <- predict(regression, rosenbrock_data)
  best_predicted <- rosenbrock_data[prediction == min(prediction), ]
  i <<- i + 1
  print(i)
  best_predicted$Y
})

simple_model_mins <- as.numeric(unlist(simple_model_mins))
simple_model_summary <- summary(simple_model_mins)
simple_model_summary
#+END_SRC

#+RESULTS:
#+begin_example
[1] 1
[1] 2
[1] 3
[1] 4
[1] 5
[1] 6
[1] 7
[1] 8
[1] 9
[1] 10
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
   1583    1583    1588    1588    1593    1593
#+end_example

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
i <- 0
modelled_mins <- replicate(100, {
  output <- optFederov(~ Var1 + Var2 + I(Var1 ^ 4) + I(Var1 ^ 2) + I(Var2 ^ 2) + I(Var1 ^ 2):Var2, data = rosenbrock_data, nTrials = 8)
  regression <- lm(Y ~ Var1 + Var2 + I(Var1 ^ 4) + I(Var1 ^ 2) + I(Var2 ^ 2) + I(Var1 ^ 2):Var2, data = output$design)
  summary.aov(regression)
  prediction <- predict(regression, rosenbrock_data)
  best_predicted <- rosenbrock_data[prediction == min(prediction), ]
  i <<- i + 1
  print(i)
  best_predicted$Y
})

modelled_mins <- as.numeric(unlist(modelled_mins))
modelled_summary <- summary(modelled_mins)
modelled_summary
#+END_SRC

#+RESULTS:
#+begin_example
[1] 1
[1] 2
[1] 3
[1] 4
[1] 5
[1] 6
[1] 7
[1] 8
[1] 9
[1] 10
[1] 11
[1] 12
[1] 13
[1] 14
[1] 15
[1] 16
[1] 17
[1] 18
[1] 19
[1] 20
[1] 21
[1] 22
[1] 23
[1] 24
[1] 25
[1] 26
[1] 27
[1] 28
[1] 29
[1] 30
[1] 31
[1] 32
[1] 33
[1] 34
[1] 35
[1] 36
[1] 37
[1] 38
[1] 39
[1] 40
[1] 41
[1] 42
[1] 43
[1] 44
[1] 45
[1] 46
[1] 47
[1] 48
[1] 49
[1] 50
[1] 51
[1] 52
[1] 53
[1] 54
[1] 55
[1] 56
[1] 57
[1] 58
[1] 59
[1] 60
[1] 61
[1] 62
[1] 63
[1] 64
[1] 65
[1] 66
[1] 67
[1] 68
[1] 69
[1] 70
[1] 71
[1] 72
[1] 73
[1] 74
[1] 75
[1] 76
[1] 77
[1] 78
[1] 79
[1] 80
[1] 81
[1] 82
[1] 83
[1] 84
[1] 85
[1] 86
[1] 87
[1] 88
[1] 89
[1] 90
[1] 91
[1] 92
[1] 93
[1] 94
[1] 95
[1] 96
[1] 97
[1] 98
[1] 99
[1] 100
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.
      0       0       0       0       0       0
#+end_example

#+HEADER: :results output :session *R* :exports none :eval no-export
#+BEGIN_SRC R
library(dplyr)

summaries <- as.data.frame(rbind(random_summary, linear_summary, modelled_summary))
summaries <- summaries[, c("Min.", "Mean", "Max.")]
summaries$Method <- c("Random Sampling", "D-Opt. w/ Linear Model", "D-Opt. w/ Correct Model")
summaries <- summaries[, c("Method", "Mean", "Min.", "Max.")]
rownames(summaries) <- NULL
summaries
write.csv(summaries, file = "data/rosenbrock_summaries.csv", row.names = FALSE)
#+END_SRC

#+RESULTS:
:                    Method       Mean         Min.     Max.
: 1         Random Sampling   133.9259     1.153125  1529.62
: 2  D-Opt. w/ Linear Model 14415.8800 14409.000000 14425.00
: 3 D-Opt. w/ Correct Model     0.0000     0.000000     0.00

* Generating Figures                                               :noexport:
** SPAPT
*** Cloning/Pulling the Repository
#+HEADER: :results output :eval no-export
#+BEGIN_SRC shell
git clone https://github.com/phrb/dlmt_spapt_experiments.git || (cd dlmt_spapt_experiments && git pull)
#+END_SRC

#+RESULTS:
: Already up to date.

*** Generate pdf
#+HEADER: :results graphics output :session *R* :eval no-export
#+HEADER: :file ./img/iteration_best_comparison.pdf
#+HEADER: :width 11 :height 16
#+BEGIN_SRC R
library(ggplot2)
library(plyr)
library(dplyr)
library(reshape2)
library(openssl)
library(RColorBrewer)
library(extrafont)

data_dir <- "dlmt_spapt_experiments/data/results"
target_dirs <- list.dirs(path = data_dir, full.names = FALSE, recursive = FALSE)
data <- NULL

read.csv.iterations.cost <- function(csv_file) {
    data <- read.csv(csv_file, header = TRUE)

    data$experiment_id <- rep(sha1(csv_file), nrow(data))
    data_baseline <- data[data$baseline == "True", "cost_mean"]
    data$cost_baseline <- rep(data_baseline, nrow(data))
    data$speedup <- data_baseline / data$cost_mean
    data$max_run_speedup <- rep(max(data$speedup), nrow(data))
    data$min_run_cost <- rep(min(data$cost_mean), nrow(data))

    data <- data[data$baseline == "False", ]
    data <- data[data$correct_result == "True", ]

    data$best_iteration <- rep(as.numeric(rownames(data[data$speedup == max(data$speedup), ])), nrow(data))
    data$points <- rep(nrow(data), nrow(data))

    return(data)
}

for (target_dir in target_dirs) {
    target_path <- paste(data_dir, "/", target_dir, "/", sep = "")

    csv_files <- list.files(path = target_path, pattern = "search_space.csv", recursive = TRUE)
    if (length(csv_files) != 0) {
        csv_files <- paste0(target_path, csv_files)

        info <- file.info(csv_files)
        non_empty <- rownames(info[info$size != 0, ])
        csv_files <- csv_files[csv_files %in% non_empty]
        target_data <- lapply(csv_files, read.csv.iterations.cost)
        target_data <- bind_rows(target_data)
        target_data <- target_data[, c("cost_mean", "experiment_id", "technique", "cost_baseline", "min_run_cost", "best_iteration")]
        target_data$application <- rep(target_dir, nrow(target_data))

        if (is.null(data)) {
            data <- target_data
        } else {
            data <- bind_rows(data, target_data)
        }
    }
}

plot_data <- data %>%
             distinct(experiment_id, .keep_all = TRUE) %>%
             group_by(application) %>%
             mutate(mean_cost_baseline = mean(cost_baseline)) %>%
             ungroup()

rs_sample <- data[data$technique == "RS", c("application", "technique", "cost_mean")]
dlmt_sample <- data[data$technique == "DLMT", c("application", "technique", "cost_mean")]

ggplot(plot_data, aes(min_run_cost, best_iteration, color = technique)) +
    facet_wrap(application ~ ., ncol = 2) +
    geom_jitter(data = rs_sample, aes(x = cost_mean, y = 300), pch = 19, alpha = 0.1, height = 85, width = 0) +
    geom_jitter(data = dlmt_sample, aes(x = cost_mean, y = 100), pch = 19, alpha = 0.1, height = 85, width = 0) +
    geom_point(size = 2, pch = 19) +
    stat_ellipse(type = "t", linetype = 13) +
    geom_vline(aes(xintercept = mean_cost_baseline), linetype = 8, color = "black") +
    scale_x_continuous(trans = "log10") +
    #coord_flip() +
    ggtitle("") +
    ylab("Iteration where Best was Found") +
    xlab("Best Cost") +
    theme_bw(base_size = 14) +
    theme(legend.position = "bottom",
          legend.direction = "horizontal",
          legend.title = element_blank(),
          text = element_text(family="Noto Serif"),
          strip.background = element_rect(fill = "white"),
          plot.margin = unit(c(0.2, 0.2, 0.2, 0.2), "cm"))  +
    scale_color_brewer(palette = "Set1")
#+END_SRC

#+RESULTS:
[[file:./img/iteration_best_comparison.pdf]]
** Rosenbrock
#+HEADER: :results graphics output :session *R* :exports none :eval no-export
#+HEADER: :file "./img/rosenbrock.pdf"
#+HEADER: :width 12 :height 12
#+BEGIN_SRC R
library(ggplot2)

rosenbrock <- function(x, y) {
    return(((1.0 - x) ^ 2) + (100.0 * ((y - (x ^ 2)) ^ 2))+ rnorm(1, sd = 10))
}

rosenbrock_data <- expand.grid(seq(-4, 4, 0.05),
                               seq(-4, 4, 0.05))

names(rosenbrock_data) <- c("x", "y")
rosenbrock_data$Y <- mapply(rosenbrock,
                            rosenbrock_data$x,
                            rosenbrock_data$y)

ggplot(rosenbrock_data, aes(x, y, z = Y)) +
      scale_x_continuous(limits = c(-4, 4), expand = c(0, 0)) +
      scale_y_continuous(limits = c(-4, 4), expand = c(0, 0)) +
      #geom_contour(colour = "black", show.legend = FALSE, breaks = 5 * (10 ^ (-1:7))) +
      geom_point(size = 4, colour = "black", pch = 19, data = rosenbrock_data[rosenbrock_data$Y == min(rosenbrock_data$Y), ]) +
      geom_label(size = 11, colour = "black", data = rosenbrock_data[rosenbrock_data$Y == min(rosenbrock_data$Y), ], aes(x = x, y = y + 0.35, label = "rosenbrock(1, 1) = 0")) +
      theme_bw(base_size = 35) +
      theme(panel.grid = element_blank(), panel.border = element_blank())
#+END_SRC

#+RESULTS:
[[file:./img/rosenbrock.pdf]]
** GPU Laplacian Kernel
#+HEADER: :file ./img/comparison_histogram.pdf :exports none :width 7 :height 8 :eval no-export
#+BEGIN_SRC R :results output graphics  :session *R*
library(ggplot2)
library(plyr)

df_all_methods <- read.csv("./data/complete_1000.csv", strip.white = T, header = T)
df_all_methods$method <- factor(df_all_methods$method, levels = c("RS","LHS","GS","GSR","GA","LM", "LMB", "LMBT", "RQ", "DOPT", "DLM", "DLMT"))
df_all_methods <- subset(df_all_methods, method %in% c("RS", "LHS", "GS", "GSR", "GA", "LM", "DLMT"))

df_mean = ddply(df_all_methods,.(method), summarize,
                mean = mean(slowdown))

df_median = ddply(df_all_methods,.(method), summarize,
                  median = median(slowdown))

df_err = ddply(df_all_methods,.(method), summarize,
              mean = mean(slowdown), err = 2 * sd(slowdown) / sqrt(length(slowdown)))

df_max = ddply(df_all_methods,.(method), summarize, max = max(slowdown))

ggplot(df_all_methods) +
    facet_grid(method ~ .) +
    theme_bw(base_size = 18) +
    coord_cartesian(xlim = c(.9, 4), ylim = c(0, 1000)) +
    geom_histogram(aes(slowdown), binwidth = .05, fill = "gray48") +
    scale_y_continuous(breaks = c(0, 1000), labels = c("0", "1000")) +
    geom_curve(data = df_max, aes(x = max + .1, y = 500, xend = max, yend = 5), arrow = arrow(length = unit(0.05, "npc")), curvature = 0.3) +
    geom_text(aes(x = max+.2, y = 550, label = "max"), data = df_max) +
    geom_rect(data = df_err, aes(xmin = mean-err, xmax = mean + err, ymin = 0, ymax = 1000, fill = "red"), alpha = 0.3) +
    geom_vline(aes(xintercept = median), df_median, color = "darkgreen", linetype = 3) +
    geom_vline(aes(xintercept = mean), df_mean, color = "red", linetype = 2) +
    labs(y = "Frequency", x = "Slowdown compared to the optimal solution") +
    scale_fill_discrete(name = "", breaks = c("red"), labels = c("Mean error")) +
    ggtitle("") +
    theme(legend.position = "none",
          text = element_text(family="Noto Serif"),
          strip.background = element_rect(fill = "white"))
#+END_SRC

#+RESULTS:
[[file:./img/comparison_histogram.pdf]]
** Representing Sampling Strategies
*** Generate Data
#+HEADER: :results output :session *R* :eval no-export
#+BEGIN_SRC R
library(ggplot2)
library(DoE.wrapper)
library(AlgDesign)
library(dplyr)
library(RColorBrewer)

sample_size <- 50
pre_sample_size <- 30 * sample_size
search_space_size <- 100

objective_data <- expand.grid(seq(0, 100, 1),
                              seq(0, 100, 1))

names(objective_data) <- c("x1", "x2")
objective_data$Y <- ((objective_data$x1 - (search_space_size / 2)) ^ 2) + ((objective_data$x2 - (search_space_size / 2)) ^ 2)

rs_data <- data.frame(x1 = sample(0:search_space_size, sample_size, replace = T),
                      x2 = sample(0:search_space_size, sample_size, replace = T))
rs_data$name <- rep("Random Sampling", nrow(rs_data))
data <- rs_data

lhs_data <- lhs.design(nruns = sample_size ,nfactors = 2, digits = 0, type = "maximin",
                       factor.names = list(x1 = c(0, search_space_size), x2 = c(0, search_space_size)))
lhs_data$name <- rep("Latin Hyper Square", nrow(lhs_data))
data <- bind_rows(data, lhs_data)

full_factorial <- gen.factorial(c(search_space_size, search_space_size), center = FALSE)
names(full_factorial) <- c("x1", "x2")
output <- optFederov(~., full_factorial, nTrials = sample_size)
dopt_data <- output$design
dopt_data$name <- rep("DOpt. Linear Model", nrow(dopt_data))
data <- bind_rows(data, dopt_data)

full_factorial <- gen.factorial(c(search_space_size, search_space_size), center = FALSE)
names(full_factorial) <- c("x1", "x2")
output <- optFederov(~ . + I(x1 ^ 2) + I(x2 ^ 2), full_factorial, nTrials = sample_size)
#output <- optFederov(~ . + quad(.), full_factorial, nTrials = sample_size)
doptq_data <- output$design
doptq_data$name <- rep("DOpt. Quadratic Model", nrow(doptq_data))
data <- bind_rows(data, doptq_data)

fake_gradient_data_seed <- data.frame(x1 = c(1, 1, 99, 99),
                                      x2 = c(1, 99, 1, 99),
                                      run = c(1, 2, 3, 4),
                                      sign1 = c(1, 1, -1, -1),
                                      sign2 = c(1, -1, 1, -1))

fake_gradient_data <- NULL

for(run_id in c(1, 2, 3, 4)) {
  if (is.null(fake_gradient_data)) {
      fake_gradient_data <- fake_gradient_data_seed[run_id, ]
  } else {
      fake_gradient_data <- rbind(fake_gradient_data, fake_gradient_data_seed[run_id, ])
  }

  for(i in 1:10) {
      row <- nrow(fake_gradient_data)
      fake_descent <- data.frame(x1 = ceiling(fake_gradient_data[row, "x1"] + (fake_gradient_data[row, "sign1"] * runif(1, min = 1, max = 5))),
                                 x2 = ceiling(fake_gradient_data[row, "x2"] + (fake_gradient_data[row, "sign2"] * runif(1, min = 1, max = 5))),
                                 run = fake_gradient_data[row, "run"],
                                 sign1 = fake_gradient_data[row, "sign1"],
                                 sign2 = fake_gradient_data[row, "sign2"])
      fake_gradient_data <- rbind(fake_gradient_data, fake_descent)
  }
}

fake_gradient_data$name <- rep("Gradient Descent", nrow(fake_gradient_data))
data <- bind_rows(data, fake_gradient_data)

fake_sima_data_seed <- data.frame(x1 = c(30, 30, 70, 70),
                                  x2 = c(30, 70, 30, 70),
                                  run = c(1, 2, 3, 4),
                                  sign1 = c(1, 1, -1, -1),
                                  sign2 = c(1, -1, 1, -1))

fake_sima_data <- NULL

for(run_id in c(1, 2, 3, 4)) {
  if (is.null(fake_sima_data)) {
      fake_sima_data <- fake_sima_data_seed[run_id, ]
  } else {
      fake_sima_data <- rbind(fake_sima_data, fake_sima_data_seed[run_id, ])
  }

  for(i in 1:10) {
      row <- nrow(fake_sima_data)
      fake_descent <- data.frame(x1 = ceiling(fake_sima_data[row, "x1"] + (fake_sima_data[row, "sign1"] * runif(1, min = -5, max = 5))),
                                 x2 = ceiling(fake_sima_data[row, "x2"] + (fake_sima_data[row, "sign2"] * runif(1, min = -5, max = 5))),
                                 run = fake_sima_data[row, "run"],
                                 sign1 = fake_sima_data[row, "sign1"],
                                 sign2 = fake_sima_data[row, "sign2"])
      fake_sima_data <- rbind(fake_sima_data, fake_descent)
  }
}

fake_sima_data$name <- rep("Simulated Annealing", nrow(fake_sima_data))
data <- bind_rows(data, fake_sima_data)
#+END_SRC

#+RESULTS:

*** Plot
#+HEADER: :results graphics output :session *R*
#+HEADER: :file ./img/sampling_comparison.pdf :exports none :width 13 :height 10 :eval no-export
#+BEGIN_SRC R
  library(extrafont)
  data$facet <- factor(data$name, levels = c("Random Sampling", "Latin Hyper Square", "Gradient Descent", "Simulated Annealing", "DOpt. Linear Model", "DOpt. Quadratic Model"))
  ggplot(data, aes(x = x1, y = x2)) +
      scale_x_continuous(limits = c(-1, 101), expand = c(0, 0)) +
      scale_y_continuous(limits = c(-1, 101), expand = c(0, 0)) +
      xlab("x") +
      ylab("y") +
      facet_wrap(facet ~ ., ncol = 3) +
      #geom_raster(data = objective_data, aes(fill = Y), show.legend = FALSE) +
      #geom_contour(data = objective_data, aes(z = Y), colour = "white", linetype = 8) + #, breaks = 1 * (2 ^ (2:20))) +
      geom_contour(data = objective_data, aes(z = Y), linetype = 1, colour = "black", alpha = 0.5, show.legend = FALSE, breaks = 1 * (2 ^ (4:20))) +
      geom_line(data = subset(data, name %in% c("Gradient Descent", "Simulated Annealing")), aes(group = run), color = "black", alpha = 0.6) +
      geom_point(shape = 19, size = 2, colour = "black", alpha = 0.6) +
      scale_fill_distiller(palette = "Greys", direction = -1, limits = c(min(objective_data$Y) - 1000, max(objective_data$Y))) +
      theme_bw(base_size = 30) +
      theme(panel.grid = element_blank(),
            text = element_text(family="Noto Serif"),
            strip.background = element_rect(fill = "white"),
            axis.text.x = element_blank(),
            axis.ticks.x = element_blank(),
            axis.text.y = element_blank(),
            axis.ticks.y = element_blank())
#+END_SRC

#+RESULTS:
[[file:./img/sampling_comparison.pdf]]
* Introduction
Optimizing code for objectives such as performance and power consumption is
fundamental to the success and cost effectiveness of industrial and scientific
endeavors in High Performance Computing. A considerable amount of highly
specialized time and effort is spent in porting and optimizing code for GPUs,
FPGAs and other hardware accelerators. Experts are also needed to leverage
bleeding edge software improvements in compilers, languages, libraries and
frameworks. The automatic configuration and optimization of High Performance
Computing applications, or /autotuning/, is a technique effective in decreasing
the cost and time needed to adopt efficient hardware and software. Typical
targets for autotuning include algorithm selection, source-to-source
transformations and compiler configuration.

Autotuning can be studied as a search problem, where the objective is to
minimize single or multiple software of hardware metrics. The exploration of the
search spaces defined by configurations and optimizations present interesting
challenges to search strategies. These search spaces grow exponentially with the
number of considered configuration parameters and their possible values. They
are also difficult to extensively explore due to the often prohibitive costs of
hardware utilization and program compilation and execution times. Developing
autotuning strategies capable of producing good optimizations while minimizing
resource utilization is therefore essential. The capability of acquiring
knowledge about an optimization problem is also a desired feature of an
autotuning strategy, since this knowledge can decrease the cost of subsequent
optimizations of the same application or for the same hardware.

It is common and usually effective to use search meta-heuristics such as genetic
algorithms and simulated annealing in autotuning. These strategies usually
attempt to exploit local properties and are not capable of fully exploiting
global search space structures. They are also not much more effective in
comparison with a naive uniform random sample of the search
space\nbsp{}\cite{seymour2008comparison,knijnenburg2003combined}, and usually rely on a
large number of measurements and frequent restarts to achieve good performance
improvements. Search strategies based on gradient descent also are commonly used
in autotuning and rely on a large number of measurements. Their effectiveness
diminishes additionally in search spaces with complex local structures.
Completely automated machine learning autotuning strategies are effective in
building models for predicting important optimization parameters, but still rely
on a sizable data set for training. Large data sets are fundamental to
strategies based on machine learning since they select models from a generally
very large class.

Search strategies based on meta-heuristics, gradient descent and machine
learning require a large number of measurements to be effective, and are usually
incapable of providing knowledge about search spaces to users. At the end of
each autotuning session it is difficult to decide if and where further
exploration is warranted, and impossible to know which parameters are
responsible for the observed improvements. After exploring a search space, it is
impossible to confidently deduce its global properties since its was explored
with unknown biases.

In this paper we propose an autotuning strategy that leverages existing expert
and approximate knowledge about a problem in the form of a performance model,
and refines this initial model iteratively using empirical performance
evaluations, statistical analysis and user input. Our strategy puts a heavy
weight on decreasing the costs of autotuning by using efficient Design of
Experiments strategies to minimize the number of experiments needed to find good
optimizations. Each optimization iteration uses /Analysis of Variance/ (ANOVA)
to help identify the relative significance of each configurable parameter to the
performance observations. An architecture- and problem-specific performance
model is built iteratively and with user input, enabling informed decisions on
which regions of the search space are worth exploring.

We present the performance of our approach on a Laplacian Kernel for GPUs where
the search space, global optimum and performance model approximation are known.
The experimental budget on this application were tightly constrained. The
speedups achieved and the budget utilization of our approach on this setting
motivated a more comprehensive performance evaluation. We chose the /Search
Problems in Automatic Performance Tuning/
(SPAPT)\nbsp{}\cite{balaprakash2012spapt} benchmark for this evaluation, where
our approach was able to find speedups of over 50$\times$ for some SPAPT
applications, finding speedups better than random sampling in some scenarios.
Despite using generic performance models for every SPAPT application, our
approach was able to significantly decrease the budget used to find performance
improvements.

The rest of this paper is organized as follows. Section [[Background]] presents
related work on source-to-source transformation, which is the main optimization
target in SPAPT problems, on autotuning systems and on search space exploration
strategies. Section [[Applying Design of Experiments to Autotuning]] presents a
detailed description of the implementation of our approach and its background.
It discusses the Design of Experiments concepts we incorporate, and the ANOVA
and linear regression algorithms we use in analysis steps. Section [[Performance
Evaluation]] presents our results with the GPU Laplacian Kernel and the SPAPT
benchmark. Section [[Conclusion]] discusses our conclusions and future work.
* Background
** Source-to-source Transformation
** Autotuning
John Rice's Algorithm Selection framework\nbsp{}\cite{rice1976algorithm} is the
precursor of autotuners in various problem domains. In 1997, the PHiPAC
system\nbsp{}\cite{bilmes1997optimizing} used code generators and search scripts
to automatically generate high performance code for matrix multiplication. Since
then, systems approached different domains with a variety of strategies.
Dongarra /et al./\nbsp{}\cite{dongarra1998automatically} introduced the ATLAS
project, that optimizes dense matrix multiplication routines. The
OSKI\nbsp{}\cite{vuduc2005oski} library provides automatically tuned kernels for
sparse matrices. The FFTW\nbsp{}\cite{frigo1998fftw} library provides tuned C
subroutines for computing the Discrete Fourier Transform.
Periscope\nbsp{}\cite{gerndt2010automatic} is a distributed online autotuner for
parallel systems and single-node performance. In an effort to provide a common
representation of multiple parallel programming models, the INSIEME compiler
project\nbsp{}\cite{jordan2012multi} implements abstractions for OpenMP, MPI and
OpenCL, and generates optimized parallel code for heterogeneous multi-core
architectures.

A different approach is to combine generic search algorithms and problem
representation data structures in a single system that enables the
implementation of autotuners for different domains. The
PetaBricks\nbsp{}\cite{ansel2009petabricks} project provides a language,
compiler and autotuner, enabling the definition and selection of multiple
algorithms for the same problem. The ParamILS
framework\nbsp{}\cite{hutter2009paramils} applies stochastic local search
algorithms to algorithm configuration and parameter tuning. The OpenTuner
framework\nbsp{}\cite{ansel2014opentuner} provides ensembles of techniques that
search the same space in parallel, while exploration is managed by an
implementation of a solver of the multi-armed bandit problem.
** Search Space Exploration Strategies
#+BEGIN_CENTER
#+CAPTION: Exploration of the search space defined by $x^2 + y^2$, using a fixed budget of 50 points
#+ATTR_LATEX: :width .95\columnwidth
[[./img/sampling_comparison.pdf]]
#+END_CENTER
* Design of Experiments
An /experimental design/ determines a selection of experiments whose objective
is to identify the relationships between /factors/ and /responses/. While
factors and responses can refer to different concrete entities in other domains,
in computer experiments factors can be configuration parameters for algorithms
and compilers, for example, and responses can be the execution time or memory
consumption of a program. Each possible value of a factor is called a /level/.

Experimental designs are constructed with objectives such as identifying the
most important factors and building an analytical model for the response. This
Section discusses some design construction techniques we explored and presents
the technique we selected for our approach.
** Screening
#+HEADER: :results output latex :session *R* :exports results
#+BEGIN_SRC R
library(FrF2)
library(xtable)

options(warn = -1)
design <- pb(8, factor.names = c("x1", "x2", "x3", "x4", "x5", "x6", "x7"))
options(warn = 0)

design$Y <- (0.1 * as.numeric(design$x1)) + (1.3 * as.numeric(design$x3)) +
            (3.1 * as.numeric(design$x5)) + (-1.4 * as.numeric(design$x7)) +
            rnorm(1, sd = 5)

regression <- lm(Y ~ ., data = design)

cap <- "Randomized Plackett-Burman design for 8 factors"
tab <- xtable(design, caption = cap)
align(tab) <- "ccccccccc"
print(tab, booktabs = TRUE, include.rownames = FALSE, caption.placement = "top", size = "\\small")
#+END_SRC

#+RESULTS:
#+BEGIN_EXPORT latex
% latex table generated in R 3.5.1 by xtable 1.8-2 package
% Thu Oct  4 21:15:27 2018
\begin{table}[ht]
\centering
\caption{Randomized Plackett-Burman design for 8 factors}
\begingroup\small
\begin{tabular}{cccccccc}
  \toprule
x1 & x2 & x3 & x4 & x5 & x6 & x7 & Y \\
  \midrule
1 & -1 & 1 & -1 & -1 & 1 & 1 & -2.16 \\
  1 & -1 & -1 & 1 & 1 & 1 & -1 & 1.04 \\
  -1 & 1 & -1 & -1 & 1 & 1 & 1 & -0.46 \\
  -1 & 1 & 1 & 1 & -1 & 1 & -1 & -0.86 \\
  1 & 1 & -1 & 1 & -1 & -1 & 1 & -3.46 \\
  1 & 1 & 1 & -1 & 1 & -1 & -1 & 2.34 \\
  -1 & -1 & -1 & -1 & -1 & -1 & -1 & -2.16 \\
  -1 & -1 & 1 & 1 & 1 & -1 & 1 & 0.84 \\
   \bottomrule
\end{tabular}
\endgroup
\end{table}
#+END_EXPORT

#+HEADER: :results graphics output :session *R* :exports none :eval no-export
#+HEADER: :file ./img/main_effects.pdf
#+HEADER: :width 8 :height 4
#+BEGIN_SRC R
library(extrafont)

par(family = 'serif')
par(mar = c(0.5,0.5,0.5,0.5))
p <- MEPlot(regression, main = NULL, pch = 16, lwd = 1, cex.xax = 1, cex.main = 1, cex.axis = 1)
p
#+END_SRC

#+RESULTS:
[[file:./img/main_effects.pdf]]

*** Analysis of Variance (ANOVA)
#+BEGIN_EXPORT latex
\begin{figure}
{\normalsize
\begin{align*}
\mathbf{Y} = \bm{\beta}\mathbf{X} + \bm{\epsilon}
\end{align*}
}
\caption{Linear model assumed in main-effect analysis of screening designs}
\end{figure}
#+END_EXPORT
**** Optimizing the Rosenbrock Function                         :noexport:
#+CAPTION: Defining the Rosenbrock function in =R=
#+BEGIN_figure
#+HEADER: :results output :session *R* :exports code
#+BEGIN_SRC R
rosenbrock <- function(x, y) {
    return(((1.0 - x) ^ 2) + (100.0 * ((y - (x ^ 2)) ^ 2)))
}
#+END_SRC

#+RESULTS:

#+END_FIGURE

#+HEADER: :results output latex :session *R* :exports results :eval no-export
#+BEGIN_SRC R
library(xtable)

cap <- "Comparison of 3 optimization methods on Rosenbrock's function, using a buget of 10 points with 100 repetitions"
rosenbrock_summaries <- read.csv(file = "./data/rosenbrock_summaries.csv", header = TRUE)
x <- xtable(rosenbrock_summaries, caption = cap, display = c("s", "s", "g", "g", "g"), digits = 2)
align(x) <- xalign(x)
print(x, size = "\\small", include.rownames = FALSE, booktabs = TRUE, math.style.exponents = TRUE, caption.placement = "top")
#+END_SRC

#+RESULTS:
#+BEGIN_EXPORT latex
% latex table generated in R 3.5.1 by xtable 1.8-2 package
% Wed Oct  3 11:29:52 2018
\begin{table}[ht]
\centering
\begingroup\small
\begin{tabular}{lrrr}
  \toprule
Method & Mean & Min. & Max. \\
  \midrule
Random Sampling & $1.3 \times 10^{2}$ & 1.2 & $1.5 \times 10^{3}$ \\
  D-Opt. w/ Linear Model & $1.4 \times 10^{4}$ & $1.4 \times 10^{4}$ & $1.4 \times 10^{4}$ \\
  D-Opt. w/ Correct Model &   0 &   0 &   0 \\
   \bottomrule
\end{tabular}
\endgroup
\caption{Comparison of 3 optimization methods on Rosenbrock's function, using a buget of 10 points with 100 repetitions}
\end{table}
#+END_EXPORT

#+BEGIN_CENTER
#+CAPTION: Contour plot in $log_{10}$ scale and global optimum of Rosenbrock's function
#+BEGIN_figure
#+ATTR_LATEX: :width .8\columnwidth
[[./img/rosenbrock.pdf]]
#+END_FIGURE
#+END_CENTER

#+HEADER: :results output latex :session *R* :exports results :eval no-export
#+BEGIN_SRC R
library(AlgDesign)
library(xtable)

rosenbrock <- function(x, y) {
    return(((1.0 - x) ^ 2) + (100.0 * ((y - (x ^ 2)) ^ 2)) + rnorm(1, sd = 10))
}

rosenbrock_data <- expand.grid(seq(-4, 4, 0.05),
                               seq(-4, 4, 0.05))

names(rosenbrock_data) <- c("x", "y")
rosenbrock_data$Y <- mapply(rosenbrock,
                            rosenbrock_data$x,
                            rosenbrock_data$y)

output <- optFederov(~ x + y + I(x ^ 4) + I(y ^ 2) + I(y ^ 2) + I(x ^ 2):y, data = rosenbrock_data, nTrials = 10)
regression <- aov(Y ~ x + y + I(x ^ 4) + I(x ^ 2) + I(y ^ 2) + I(x ^ 2):y, data = output$design)
s_regression <- as.data.frame(summary.aov(regression)[[1]])
s_regression <- s_regression[1:6, c("F value", "Pr(>F)")]

cap <- "Shortened ANOVA table for the fit of the correct model using 10 experiments"
x <- xtable(s_regression, caption = cap, display = c("s","g", "g"), digits = 2)
align(x) <- xalign(x)
print(x, size = "\\small", math.style.exponents = TRUE, booktabs = TRUE, caption.placement = "top")
#+END_SRC

#+RESULTS:
#+BEGIN_EXPORT latex
% latex table generated in R 3.5.1 by xtable 1.8-2 package
% Wed Oct  3 16:57:45 2018
\begin{table}[ht]
\centering
\caption{Shortened ANOVA table for the fit of the correct model using 10 experiments}
\begingroup\small
\begin{tabular}{lrr}
  \toprule
 & F value & Pr($>$F) \\
  \midrule
x           & $2 \times 10^{4}$ & $7.5 \times 10^{-7}$ \\
  y           & $9.2 \times 10^{6}$ & $7.9 \times 10^{-11}$ \\
  I(x\verb|^|4)      & $2 \times 10^{7}$ & $2.4 \times 10^{-11}$ \\
  I(x\verb|^|2)      & $3.3 \times 10^{5}$ & $1.2 \times 10^{-8}$ \\
  I(y\verb|^|2)      & $3 \times 10^{4}$ & $4.2 \times 10^{-7}$ \\
  y:I(x\verb|^|2)    & $3.9 \times 10^{6}$ & $2.8 \times 10^{-10}$ \\
   \bottomrule
\end{tabular}
\endgroup
\end{table}
#+END_EXPORT

#+HEADER: :results output latex :session *R* :exports results :eval no-export
#+BEGIN_SRC R
library(AlgDesign)
library(xtable)

rosenbrock <- function(x, y) {
    return(((1.0 - x) ^ 2) + (100.0 * ((y - (x ^ 2)) ^ 2)))
}

rosenbrock_data <- expand.grid(seq(-4, 4, 0.05),
                               seq(-4, 4, 0.05))

names(rosenbrock_data) <- c("x", "y")
rosenbrock_data$Y <- mapply(rosenbrock,
                            rosenbrock_data$x,
                            rosenbrock_data$y)

output <- optFederov(~ ., data = rosenbrock_data, nTrials = 10)
regression <- lm(Y ~ ., data = output$design)
s_regression <- as.data.frame(summary.aov(regression)[[1]])
s_regression <- s_regression[1:2, c("F value", "Pr(>F)")]

cap <- "Shortened ANOVA table for the fit of the naive linear model using 10 experiments"
x <- xtable(s_regression, caption = cap, display = c("s","g", "g"), digits = 2)
align(x) <- xalign(x)
print(x, size = "\\small", math.style.exponents = TRUE, booktabs = TRUE, caption.placement = "top")
#+END_SRC

#+RESULTS:
#+BEGIN_EXPORT latex
% latex table generated in R 3.5.1 by xtable 1.8-2 package
% Wed Oct  3 13:38:41 2018
\begin{table}[ht]
\centering
\caption{Shortened ANOVA table for the fit of the naive linear model using 10 experiments}
\begingroup\small
\begin{tabular}{lrr}
  \toprule
 & F value & Pr($>$F) \\
  \midrule
x           & $7.5 \times 10^{-6}$ &   1 \\
  y           & 1.4 & 0.27 \\
   \bottomrule
\end{tabular}
\endgroup
\end{table}
#+END_EXPORT
** Multi-Level Design Construction Techniques
The application of Design of Experiments to autotuning problems requires design
construction techniques that support factors of different types and number of
possible values. Autotuning problems typically combine factors such as binary
flags, integer and floating point numerical values, and unordered enumerations
of abstract values. Minimizing the number of experiments needed to find good
optimizations is a also a fundamental requirement since we are interested in
autotuning for scenarios with tight budget constraints.

The design construction techniques that fit these requirements are limited.
Designs that simply test all possible factor combinations, or /full factorial
designs/, would provide complete information about the global minimum but are
unfeasible for most autotuning problems. In the /2-level screening with random
level sampling/ technique, factors with more than two unordered levels are
sampled at two random levels. This enables using small design such as the
Plackett-Burman\nbsp{}\cite{plackett1946design} screening design. Advantages are
the small design size and good estimation capability for main effects.
Incapability of estimating interactions is a disadvantage of this strategy, but
the main drawback is the lack of information for levels not selected in the
initial screening.

In /contractive replacement/, an initial 2-level design is used to generate
mixed-level designs by re-encoding columns into a new single column representing
a multi-level factor. The contractive replacement of
Addelman-Kempthorne\nbsp{}\cite{addelman1961some} is a strategy of this kind.
Advantages of this technique are the small design sizes and the ability to
estimate main effects. Additionally, the contractive replacement technique
preserves orthogonality. Due to strict requirements on initial designs, not all
2-level designs can be contracted.

The /direct generation/ algorithm presented by Grömping and
Fontana\nbsp{}\cite{ulrike2018algorithm} enables the generation of multi-level
designs by solving Mixed Integer Problems (MIP). The advantages of this
technique are the direct generation of multi-level designs and a clearly defined
optimality criterion. Since this construction relies on solving carefully
formulated MIP problems, it presents strong restrictions on the size and shape
of the designs that can be generated.
** D-Optimal Designs
/D-Optimal designs/ are the class of designs that best fits our requirements of
supporting multi-level factors and minimizing the number of experiments. The
algorithms for constructing D-Optimal designs are relatively fast and have few
restrictions.

It is necessary to select a model that relates factors and responses to
construct a D-Optimal design. The model selection can be based on previous
experiments or on expert knowledge of the problem. Once a model is selected,
algorithmic construction is performed by searching for the set of experiments
that minimizes the /D-Optimality/ criterion, a measure of the /variance/ of the
/estimators/ of the /regression coefficients/ associated with the selected
model. This search is usually done by swapping experiments from the current
candidate set with experiments from a pool of possible experiments, according to
certain rules, until some stopping criterion is met. In the approach presented
in this paper we used Fedorov's algorithm\nbsp{}\cite{fedorov1972theory} for
constructing D-Optimal designs, implemented in =R= in the =AlgDesign= package.

Considering that we are going to analyze the results of an experiments plan, the
/D-Efficiency/ of a design is inversely proportional to the /geometric mean/ of
the /eigenvalues/ of the plan's /covariance matrix/. A D-Optimal design has the
best D-Efficiency. Our current approach is based on D-Optimal designs.
* Applying Design of Experiments to Autotuning
** The DLMT Strategy
#+BEGIN_CENTER
#+ATTR_LATEX: :width .8\columnwidth
#+ATTR_ORG: :width 400
[[./img/doe_anova_strategy.pdf]]
#+END_CENTER
* Performance Evaluation
** Example on a GPU Laplacian Kernel
#+BEGIN_EXPORT latex
\begin{figure}
{\scriptsize
\begin{align*}
\texttt{time\_per\_pixel} \thicksim & \; \texttt{y\_component\_number} + 1 / \texttt{y\_component\_number} \; + \\
& \; \texttt{vector\_length} + \texttt{lws\_y} + 1 / \texttt{lws\_y} \; + \\
& \; \texttt{load\_overlap} + \texttt{temporary\_size} \; + \\
& \; \texttt{elements\_number} + 1 / \texttt{elements\_number} \; + \\
& \; \texttt{threads\_number} + 1 /\texttt{threads\_number}
\end{align*}
}
\caption{Initial performance model used by LM and DLMT}
\end{figure}
#+END_EXPORT

#+ATTR_LATEX: :booktabs t :align ll :font \footnotesize :float t :placement [ht]
#+CAPTION: Algorithms compared in the GPU Laplacian Kernel
|------+-----------------------------|
|      | Algorithm                   |
|------+-----------------------------|
| RS   | Random Sampling             |
| LHS  | Latin Hyper Square Sampling |
| GS   | Greedy Search               |
| GSR  | Greedy Search w/ Restart    |
| GA   | Genetic Algorithm           |
| LM   | Iterative Linear Model      |
| DLMT | D-Optimal Designs           |
|------+-----------------------------|

*** Results
#+HEADER: :results output latex :session *R* :exports results
#+BEGIN_SRC R
library(xtable)

df_all_methods <- read.csv("./data/complete_1000.csv", strip.white = T, header = T)
df_all_methods$method <- factor(df_all_methods$method, levels = c("RS","LHS","GS","GSR","GA","LM", "LMB", "LMBT", "RQ", "DOPT", "DLM", "DLMT"))
df_all_methods <- subset(df_all_methods, method %in% c("RS", "LHS", "GS", "GSR", "GA", "LM", "DLMT"))

summaries <- data.frame(RS = c(as.data.frame(unclass(summary(df_all_methods[df_all_methods$method == "RS", ]$slowdown)))[ , 1],
                              mean(df_all_methods[df_all_methods$method == "RS",]$point_number),
                              max(df_all_methods[df_all_methods$method == "RS",]$point_number)),
                        LHS = c(as.data.frame(unclass(summary(df_all_methods[df_all_methods$method == "LHS", ]$slowdown)))[ , 1],
                                mean(df_all_methods[df_all_methods$method == "LHS",]$point_number),
                                max(df_all_methods[df_all_methods$method == "LHS",]$point_number)),
                        GS = c(as.data.frame(unclass(summary(df_all_methods[df_all_methods$method == "GS", ]$slowdown)))[ , 1],
                              mean(df_all_methods[df_all_methods$method == "GS",]$point_number),
                              max(df_all_methods[df_all_methods$method == "GS",]$point_number)),
                        GSR = c(as.data.frame(unclass(summary(df_all_methods[df_all_methods$method == "GSR", ]$slowdown)))[ , 1],
                                mean(df_all_methods[df_all_methods$method == "GSR",]$point_number),
                                max(df_all_methods[df_all_methods$method == "GSR",]$point_number)),
                        GA = c(as.data.frame(unclass(summary(df_all_methods[df_all_methods$method == "GA", ]$slowdown)))[ , 1],
                              mean(df_all_methods[df_all_methods$method == "GA",]$point_number),
                              max(df_all_methods[df_all_methods$method == "GA",]$point_number)),
                        LM = c(as.data.frame(unclass(summary(df_all_methods[df_all_methods$method == "LM", ]$slowdown)))[ , 1],
                              mean(df_all_methods[df_all_methods$method == "LM",]$point_number),
                              max(df_all_methods[df_all_methods$method == "LM",]$point_number)),
                        DLMT = c(as.data.frame(unclass(summary(df_all_methods[df_all_methods$method == "DLMT", ]$slowdown)))[ , 1],
                                    mean(df_all_methods[df_all_methods$method == "DLMT",]$point_number),
                                    max(df_all_methods[df_all_methods$method == "DLMT",]$point_number)))

rownames(summaries) <- c(rownames(as.data.frame(unclass(summary(df_all_methods[df_all_methods$method == "RS", ]$slowdown)))), "Mean Points", "Max Points")
summaries <- t(summaries)
summaries <- summaries[ , c("Mean", "Min.", "Max.", "Mean Points", "Max Points")]

cap <- "Slowdown and budget used by 7 optimization methods on the Laplacian Kernel, using a budget of 125 points with 1000 repetitions"
x <- xtable(summaries, caption = cap, digits = 2)
align(x) <- xalign(x)
display(x) <- display(x)
print(x, size = "\\small", booktabs = TRUE, math.style.exponents = TRUE, caption.placement = "top")
#+END_SRC

#+RESULTS:
#+BEGIN_EXPORT latex
% latex table generated in R 3.5.1 by xtable 1.8-2 package
% Thu Oct  4 17:04:18 2018
\begin{table}[ht]
\centering
\caption{Comparison of slowdown and budget used by 7 optimization methods on the Laplacian Kernel, using a budget of 125 points with 1000 repetitions}
\begingroup\small
\begin{tabular}{lrrrrr}
  \toprule
 & Mean & Min. & Max. & Mean Points & Max Points \\
  \midrule
RS & 1.10 & 1.00 & 1.39 & 120.00 & 120.00 \\
  LHS & 1.17 & 1.00 & 1.52 & 98.92 & 125.00 \\
  GS & 6.46 & 1.00 & 124.76 & 22.17 & 106.00 \\
  GSR & 1.23 & 1.00 & 3.16 & 120.00 & 120.00 \\
  GA & 1.12 & 1.00 & 1.65 & 120.00 & 120.00 \\
  LM & 1.02 & 1.01 & 3.77 & 119.00 & 119.00 \\
  DLMT & 1.01 & 1.01 & 1.01 & 54.84 & 56.00 \\
   \bottomrule
\end{tabular}
\endgroup
\end{table}
#+END_EXPORT

#+BEGIN_CENTER
#+CAPTION: Histograms of 7 optimization methods on the Laplacian Kernel, using a budget of 125 points with 1000 repetitions
#+ATTR_LATEX: :float t :placement [ht] :width .9\columnwidth
[[./img/comparison_histogram.pdf]]
#+END_CENTER
** Results on the SPAPT Benchmark
*** The SPAPT Benchmark
#+ATTR_LATEX: :booktabs t :align llll :font \scriptsize :float t :placement [ht]
#+CAPTION: Set of applications we used from the SPAPT benchmark
|---------------+---------------------------------+---------+----------------------|
| Kernel        | Operation                       | Factors | Size                 |
|---------------+---------------------------------+---------+----------------------|
| =atax=        | Matrix transp. & vector mult.   |      18 | $2.6 \times 10^{16}$ |
| =dgemv3=      | Scalar, vector & matrix mult.   |      49 | $3.8 \times 10^{36}$ |
| =gemver=      | Vector mult. & matrix add.      |      24 | $2.6 \times 10^{22}$ |
| =gesummv=     | Scalar, vector, & matrix mult.  |      11 | $5.3 \times 10^{9}$  |
| =hessian=     | Hessian computation             |       9 | $3.7 \times 10^{7}$  |
| =mm=          | Matrix multiplication           |      13 | $1.2 \times 10^{12}$ |
| =mvt=         | Matrix vector product & transp. |      12 | $1.1 \times 10^{9}$  |
| =tensor=      | Tensor matrix mult.             |      20 | $1.2 \times 10^{19}$ |
| =trmm=        | Triangular matrix operations    |      25 | $3.7 \times 10^{23}$ |
| =bicg=        | Subkernel of BiCGStab           |      13 | $3.2 \times 10^{11}$ |
| =lu=          | LU decomposition                |      14 | $9.6 \times 10^{12}$ |
| =adi=         | Matrix sub., mult., & div.      |      20 | $6.0 \times 10^{15}$ |
| =jacobi=      | 1-D Jacobi computation          |      11 | $5.3 \times 10^{9}$  |
| =seidel=      | Matrix factorization            |      15 | $1.3 \times 10^{14}$ |
| =stencil3d=   | 3-D stencil computation         |      29 | $9.7 \times 10^{27}$ |
| =correlation= | Correlation computation         |      21 | $4.5 \times 10^{17}$ |
|---------------+---------------------------------+---------+----------------------|
*** Experimental Methodology
*** Results
#+BEGIN_CENTER
#+CAPTION: Results
#+ATTR_LATEX: :float multicolumn :placement [p] :width .9\textwidth
[[./img/iteration_best_comparison.pdf]]
#+END_CENTER
\clearpage
* Conclusion
* Acknowledgment
:PROPERTIES:
:UNNUMBERED: t
:END:
#+LATEX: \bibliographystyle{IEEEtran}
#+LATEX: \bibliography{references}
